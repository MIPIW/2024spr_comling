{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bac934cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "import pandas as pd \n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "from functools import reduce, partial\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "from multiprocessing import Pool\n",
    "class IOUtils():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def existsFile(file_url):\n",
    "        \n",
    "        import os\n",
    "        from pathlib import Path\n",
    "\n",
    "        if type(file_url) == str:\n",
    "            url = Path(file_url).resolve()\n",
    "        else:\n",
    "            url = file_url.resolve()\n",
    "        \n",
    "        if url.exists():\n",
    "            c = 1\n",
    "            while True:\n",
    "                if Path(str(url.parent) + \"/\" + str(url.stem) + f\"_{c}\" + str(url.suffix)).exists():\n",
    "                    c += 1\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            return Path(str(url.parent) + \"/\" + str(url.stem) + f\"_{c}\" + str(url.suffix))\n",
    "        \n",
    "        print(f\"The url of the fIle to be saved is {url}\")\n",
    "        return url\n",
    "    \n",
    "    @staticmethod\n",
    "    def recentFile(file_url):\n",
    "        \n",
    "        import os\n",
    "        from pathlib import Path\n",
    "\n",
    "        if type(file_url) == str:\n",
    "            url = Path(file_url).resolve()\n",
    "        else:\n",
    "            url = file_url.resolve()\n",
    "\n",
    "        if url.exists():\n",
    "            c = 1\n",
    "            while True:\n",
    "                if Path(str(url.parent) + \"/\" + str(url.stem) + f\"_{c}\" + str(url.suffix)).exists():\n",
    "                    c += 1\n",
    "                else:\n",
    "                    break\n",
    "        else:\n",
    "            c = 0\n",
    "        if c == 0:\n",
    "            return f\"File not exists in {url}\"\n",
    "\n",
    "        if c == 1:\n",
    "            print(f'The url of the fIle to be loaded is {Path(str(url.parent) + \"/\" + str(url.stem + str(url.suffix)))}')\n",
    "            return Path(str(url.parent) + \"/\" + str(url.stem + str(url.suffix)))\n",
    "        else:\n",
    "            print(f'The url of the fIle to be loaded is {Path(str(url.parent) + \"/\" + str(url.stem + f\"_{c-1}\" + str(url.suffix)))}')\n",
    "            return Path(str(url.parent) + \"/\" + str(url.stem + f\"_{c-1}\" + str(url.suffix)))\n",
    "\n",
    "    @staticmethod\n",
    "    def checkpoint_save(file_path, data, \n",
    "                        data_type = \"dataFrame\", \n",
    "                        file_type = \"csv\", \n",
    "                        index_dataFrame = False):\n",
    "        import json\n",
    "\n",
    "        save_path = IOUtils.existsFile(file_path)\n",
    "        \n",
    "        if data_type == \"dataFrame\" or data_type == \"series\":\n",
    "            import pandas as pd\n",
    "\n",
    "            if file_type == \"csv\":\n",
    "                data.to_csv(save_path, index = index_dataFrame, encoding = 'utf-8')\n",
    "                                \n",
    "        \n",
    "        if data_type == \"list\":\n",
    "            if file_type == \"txt\":\n",
    "                with open(save_path, \"w\", encoding = 'utf-8') as f:\n",
    "                    for item in data:\n",
    "                        f.write(item)\n",
    "                        f.write(\"\\n\")\n",
    "\n",
    "                \n",
    "            if file_type == \"jsonl\":\n",
    "                \n",
    "                with open(save_path, \"w\", encoding = 'utf-8') as f:\n",
    "                    for item in data.items():\n",
    "                        f.write(json.dumps(item))\n",
    "                        f.write(\"\\n\")\n",
    "\n",
    "        \n",
    "        if data_type == \"dict\":\n",
    "            if file_type == \"json\":\n",
    "                \n",
    "                \n",
    "                with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(data, f, indent = \"\\t\")\n",
    "        \n",
    "        return data # for the neatness of the code\n",
    "    \n",
    "    @staticmethod\n",
    "    def checkpoint_load(file_path, \n",
    "                        data_type = \"dataFrame\", \n",
    "                        file_type = \"csv\"):\n",
    "        load_path = IOUtils.recentFile(file_path)\n",
    "        \n",
    "        if data_type == \"dataFrame\" or data_type == \"series\":\n",
    "            import pandas as pd\n",
    "\n",
    "            if file_type == \"csv\":\n",
    "                out = pd.read_csv(load_path)\n",
    "                return out\n",
    "\n",
    "        if data_type == \"list\":\n",
    "            if file_type == \"txt\":\n",
    "                out = None\n",
    "\n",
    "                with open(load_path, \"r\", encoding = 'utf-8') as f:\n",
    "                    out = [i.strip() for i in f.readlines()]\n",
    "                return out\n",
    "\n",
    "            if file_type == \"jsonl\":\n",
    "                out = []\n",
    "\n",
    "                with open(load_path, \"r\", encoding = 'utf-8') as f:\n",
    "                    for line in f:\n",
    "                        out.append(json.loads(line))\n",
    "\n",
    "                return out\n",
    "\n",
    "        \n",
    "        if data_type == \"dict\":\n",
    "            if file_type == \"json\":\n",
    "                \n",
    "                out = None\n",
    "                \n",
    "                with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(data, f, indent = \"\\t\")\n",
    "                \n",
    "                with open(load_path, \"r\") as f:\n",
    "                    out = json.loads(f)\n",
    "                \n",
    "                return out\n",
    "                \n",
    "\n",
    "# 나중에 하기\n",
    "# def decorator(func, checkFunc):\n",
    "#     out = func()\n",
    "#     filter = checkFunc()\n",
    "    \n",
    "\n",
    "                \n",
    "\n",
    "class ParallelizingUtils():\n",
    "    \n",
    "    import multiprocessing\n",
    "    \n",
    "\n",
    "    def __init__(self, func):\n",
    "        self.func = func\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def do_series(self, series, num_cores, pre_assign = False):\n",
    "        if num_cores == 1:\n",
    "            if not pre_assign:\n",
    "                return self._assign_map(series)\n",
    "            else:\n",
    "                return self.func(series)\n",
    "    \n",
    "        from multiprocessing import Pool\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "\n",
    "        se_split = np.array_split(series, num_cores)\n",
    "        pool = Pool(num_cores)\n",
    "        if not pre_assign:\n",
    "            df = pd.concat(pool.map(self._assign_map, se_split))\n",
    "        else:\n",
    "            df = pd.concat(pool.map(self.func, se_split))\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        return df\n",
    "\n",
    "    def _assign_map(self, serie):\n",
    "        return serie.progress_map(self.func)\n",
    "\n",
    "    def changeFunc(self, func):\n",
    "        self.func = func\n",
    "    \n",
    "\n",
    "\n",
    "class CheckUtils():\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "    \n",
    "    # def checkValue(func):\n",
    "    #     from functools import partial\n",
    "    #     partial \n",
    "    @staticmethod\n",
    "    def checkSeries(serdf, \n",
    "                 isNan = True, \n",
    "                 isEmpty = True, \n",
    "                 isInf = True,\n",
    "                    isNotInstance = None,\n",
    "                    isNotIn = False,\n",
    "                    isNotInVal = None\n",
    "                 ):\n",
    "        \n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import math\n",
    "            \n",
    "        def _check_subsequent(x, \n",
    "                              isNotInstance = isNotInstance, \n",
    "                              isNan = isNan, \n",
    "                              isEmpty = isEmpty, \n",
    "                              isInf = isInf, \n",
    "                              isnotIn = isNotIn, \n",
    "                              isNotInVal = isNotInVal):\n",
    "            out = False\n",
    "            \n",
    "            if isNotInstance is not None:\n",
    "                x1 = (not isinstance(x, isNotInstance))\n",
    "            else:\n",
    "                x1 = False\n",
    "\n",
    "            if isinstance(x, (int, float, complex)):\n",
    "                x2 = pd.isna(x) if isNan else False\n",
    "                x3 = math.isinf(x) if isInf else False\n",
    "                x4 = False\n",
    "                \n",
    "            else:\n",
    "                x2 = False\n",
    "                x3 = False\n",
    "                x4 = (len(x) == 0) if isEmpty else False\n",
    "                \n",
    "            x5 = isNotInVal not in x if isNotIn else False\n",
    "\n",
    "            return out | x1 | x2 | x3 | x4 | x5\n",
    "        \n",
    "        \n",
    "        if isinstance(serdf, pd.Series):\n",
    "            return serdf.progress_map(lambda x: _check_subsequent(x))\n",
    "        \n",
    "        elif isinstance(serdf, pd.DataFrame):\n",
    "            out  = serdf.map(lambda x: _check_subsequent(x)) # cellwise\n",
    "\n",
    "            return out.apply(lambda x: any(x), axis = 1)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"this is not either series or dataframe.\")\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def isEmpty(dataframe) -> bool:\n",
    "    \n",
    "        if len(dataframe.index) == 0:\n",
    "            return True\n",
    "    \n",
    "        return False\n",
    "    \n",
    "#     @staticmethod\n",
    "#     def checkValue(inputs, \n",
    "# #                    isNone = True, \n",
    "#                    isNan = True, \n",
    "#                    isEmpty = True, \n",
    "#                    isInf = True,\n",
    "#                    isNotInstance = None\n",
    "#                    ):\n",
    "    \n",
    "#         import pandas as pd\n",
    "#         import numpy as np\n",
    "#         import math\n",
    "\n",
    "#         def _check_subsequent(x, isNotInstance = isNotInstance, isNan = isNan, isEmpty = isEmpty, isInf = isInf):\n",
    "#             out = False\n",
    "            \n",
    "#             if isNotInstance is not None:\n",
    "#                 x1 = (isinstance(x, isNotInstance))\n",
    "#             else:\n",
    "#                 x1 = False\n",
    "\n",
    "#             if isinstance(x, (int, float, complex)):\n",
    "#                 x2 = pd.isna(x) if isNan else False\n",
    "#                 x3 = math.isinf(x) if isInf else False\n",
    "#                 x4 = False\n",
    "                \n",
    "#             else:\n",
    "#                 x2 = False\n",
    "#                 x3 = False\n",
    "#                 x4 = (len(x) == 0) if isEmpty else False\n",
    "\n",
    "#             return out | x1 | x2 | x3 | x4\n",
    "\n",
    "#         return _check_subsequent(x, isNotInstance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "919660d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# #\\ preprocessing\n",
    "\n",
    "class Semiprocessing():\n",
    "    \n",
    "    # remove abbreviations -> pass\n",
    "    # remove string containing numbersfro\n",
    "    f1 = lambda x: re.sub(r\"[a-z]?\\d+[a-z]+\", \"\", x.lower())\n",
    "    # remove references to users/\n",
    "    f2 = lambda x: re.sub(r\"u/.+\", \"\", x)\n",
    "    # remove SRs\n",
    "    f3 = lambda x: re.sub(r\"r/.+\", \"\", x)\n",
    "    # remove full and shorteded hyperlinks\n",
    "    f4 = lambda x: re.sub(\"https?://\\S+\", \"\", x)\n",
    "    # convert British English spelling variants to American English -> pass\n",
    "    # lemmatize to remove inflectional morphology\n",
    "    f5 = lambda x: re.sub(\"(.)\\1{2,}\", \"\\1\\1\", x)\n",
    "    f6 = lambda x: x.translate(str.maketrans('', '', string.punctuation))\n",
    "    f7 = lambda x: re.sub(\"(\\s){2,}\", \" \", x)\n",
    "\n",
    "    \n",
    "    # reducing repretitions of more than three letters to three letters\n",
    "        \n",
    "    def __init__(self, load_spacy = True):\n",
    "        pass\n",
    "        \n",
    "    def process_assign(self, inp):\n",
    "        return inp.progress_map(lambda x: Semiprocessing.f7(Semiprocessing.f6(Semiprocessing.f5(Semiprocessing.f4(Semiprocessing.f3(Semiprocessing.f2(Semiprocessing.f1(x))))))))\n",
    "    \n",
    "    def progess(self, inp):\n",
    "        return Semiprocessing.f7(Semiprocessing.f6(Semiprocessing.f5(Semiprocessing.f4(Semiprocessing.f3(Semiprocessing.f2(Semiprocessing.f1(inp)))))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd3e8cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MakePrefixStructs():\n",
    "    def __init__(self, prefix_path):\n",
    "        \n",
    "        self.prefix_list = open(IOUtils.recentFile(prefix_path), \"r\").read().split(\"\\n\")        \n",
    "        self.prefix_hash = self._make_char_dict(self.prefix_list)\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def _make_char_dict(self, str_list):\n",
    "\n",
    "        str_list = sorted(str_list)\n",
    "\n",
    "        def isempty(dic, word, idx, max_len):\n",
    "\n",
    "            if idx == max_len-1:\n",
    "                dic[word[idx]] = 1 \n",
    "                return dic\n",
    "\n",
    "            if dic.get(word[idx], 0) == 0:\n",
    "                dic[word[idx]] = isempty(dict(), word, idx+1, max_len)\n",
    "\n",
    "            return dic\n",
    "\n",
    "        str_dic = {}\n",
    "        for i in str_list:\n",
    "            str_dic = isempty(str_dic, i, 0, len(i))\n",
    "\n",
    "        return str_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "547fc188",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaMaker():\n",
    "    \n",
    "    def __init__(self, prefix_hash, spacy_type, load_spacy = True):\n",
    "        self.prefix_hash = prefix_hash\n",
    "                \n",
    "        self.loaded_spacy = False\n",
    "        self.load_model = None\n",
    "        \n",
    "        if load_spacy:\n",
    "            import re, spacy\n",
    "            self.load_model = spacy.load(spacy_type, disable = ['parser', 'ner'])\n",
    "            self.loaded_spacy = True\n",
    "            \n",
    "        pass\n",
    "    \n",
    "    def lemmatize_assign(self, inputs):\n",
    "        if self.load_model is None:\n",
    "            raise ValueError(\"spacy model is not loaded. use 'load_spacy_model' to the instances\")\n",
    "        else:    \n",
    "            return inputs.progress_map(lambda x: self.load_model(x))\n",
    "    \n",
    "    def lemmatize(self, inputs):\n",
    "        if self.load_model is None:\n",
    "            raise ValueError(\"spacy model is not loaded. use 'load_spacy_model' to the instances\")\n",
    "        else:\n",
    "            return self.load_model(inputs)  \n",
    "            \n",
    "        \n",
    "    #     def pp_for_derivation(x, prefix_list):\n",
    "    def find_lemma_wo_prefix(self, x, prefix_list):\n",
    "        lst_token = [\"\"]\n",
    "        lst_token = [i.text for i in x if (not i.is_stop) and self._wordInDict(i.text, 0, self.prefix_hash)]\n",
    "        # 이 렘마도 prefix 떼고 dictionary에 넣어야 하는 거 아닌가?\n",
    "        return {\"prefixed_tokens\": lst_token, \"lemma\": \" \".join([i.lemma_ for i in x if (not i.is_stop) and i.is_alpha])}\n",
    "\n",
    "    def find_prefix_with_existing_lemma(self, x, prefix_list):\n",
    "        x = x.split(\" \")\n",
    "        lst_token = [i for i in x if self._wordInDict(i, 0, self.prefix_hash)]\n",
    "        return {\"prefixed_tokens\": lst_token}\n",
    "        \n",
    "    def _wordInDict(self, word, curlen, prefix_hash):\n",
    "        \n",
    "        if curlen == len(word):\n",
    "            return False\n",
    "        \n",
    "        if isinstance(prefix_hash.get(word[curlen], 0), dict):\n",
    "            return self._wordInDict(word, curlen + 1, prefix_hash[word[curlen]])\n",
    "        \n",
    "        if prefix_hash.get(word[curlen], 0) == 1:\n",
    "            return True\n",
    "        \n",
    "        if prefix_hash.get(word[curlen], 0) == 0:\n",
    "            return False\n",
    "        \n",
    "        \n",
    "    def load_spacy_model(self, spacy_type):\n",
    "        \n",
    "        if not self.loaded_spacy:\n",
    "            self.load_model = spacy.load(spacy_type, disable = ['parser', 'ner'])\n",
    "            self.loaded_spacy = True\n",
    "        else:\n",
    "            print(\"spacy model is already loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b30b3ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    \n",
    "class BinsMaker():\n",
    "    def __init__(self, lemma_frequent, prefix_list, load_model):\n",
    "        self.lemma_frequent = list(lemma_frequent)\n",
    "        self.prefix_list = prefix_list\n",
    "        self.load_model = load_model\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def isRealPrefix(self, word_list):\n",
    "        \n",
    "        def eachWord(word):\n",
    "            p = \"\"\n",
    "            \n",
    "            for prefix in self.prefix_list:\n",
    "                if word.startswith(prefix):\n",
    "                    p = prefix\n",
    "                    break\n",
    "                    \n",
    "        #     if load_model(word)[0].lemma_ in lemma_frequent:  # according to the paper, but wrong\n",
    "        #         return (word, prefix)\n",
    "        \n",
    "            if len(word[len(p):]) > 0:\n",
    "                lemma = self.load_model(word[len(p):])[0].lemma_\n",
    "                \n",
    "                if lemma in self.lemma_frequent:\n",
    "                    return (word, prefix, lemma)\n",
    "                \n",
    "                \n",
    "        return [eachWord(i) for i in word_list]\n",
    "        \n",
    "            \n",
    "    @staticmethod\n",
    "    def postprocess(series):\n",
    "        x = series.explode()\n",
    "        x = x[x.map(lambda x: isinstance(x, tuple))]\n",
    "        out = pd.concat([pd.Series(x.index, name = \"idx\"), pd.DataFrame.from_records(x.values, columns = [\"token\", \"prefix\", \"lemma\"])], axis = 1)\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def word_to_blank(prefix_series, article_series):\n",
    "        idx_lst = list(prefix_series.idx.drop_duplicates())\n",
    "        pre_idx_token = prefix_series.groupby('idx')['token'].apply(list)\n",
    "        pre_idx_prefix = prefix_series.groupby('idx')['prefix'].apply(list)\n",
    "        article_series = article_series[idx_lst]\n",
    "\n",
    "        for idx in tqdm(idx_lst):\n",
    "\n",
    "            x = article_series[idx].split()\n",
    "            y = pre_idx_token[idx]\n",
    "            z = pre_idx_prefix[idx]\n",
    "\n",
    "            x.append(\"[CROP]\")\n",
    "\n",
    "            for i in range(len(x)):\n",
    "                for prefix, word in zip(z, y):\n",
    "                    if x[i] == word:\n",
    "\n",
    "                        x.append(\"[AND]\".join([prefix, word]))\n",
    "                        x.append(\"[ANDMORE]\")\n",
    "\n",
    "                        x[i] = \"___\"\n",
    "\n",
    "            article_series[idx] = \" \".join(x)\n",
    "\n",
    "        return article_series\n",
    "\n",
    "    @staticmethod\n",
    "    def split_sentence(article_series):\n",
    "        import re \n",
    "\n",
    "        def split_string(string):\n",
    "\n",
    "            lst_whole = string.split('[CROP]')\n",
    "            lst_sent = lst_whole[0].split(\"___\")\n",
    "            lst_prefix_word = [i.split(\"[AND]\") for i in lst_whole[1].split(\"[ANDMORE]\")]\n",
    "\n",
    "            lst_final = []\n",
    "\n",
    "            for idx in range(len(lst_sent)-1):\n",
    "                t = lst_prefix_word[idx][0].strip()\n",
    "                word = lst_prefix_word[idx][1].strip()\n",
    "                base = word[len(t):]\n",
    "                temp1 = lst_sent[idx].strip()\n",
    "                temp2 = lst_sent[idx+1].strip()\n",
    "                a = temp1[-150:-50]\n",
    "                b = temp1[-50:] + \" ___ \" + temp2[:50]\n",
    "                c = temp2[50:200]\n",
    "                \n",
    "                lst_final.append(\"asdf\" + \"|||\" + word + \"|||\" + base + \"|||\" + a + \"|||\" + b + \"|||\" + c)\n",
    "\n",
    "            return lst_final\n",
    "\n",
    "        return article_series.progress_map(split_string).explode().reset_index(drop = True)\n",
    "\n",
    "    ###############lemma 기준으로 count해야됨 기말때 보완하기로\n",
    "\n",
    "    @staticmethod\n",
    "    def classify_bins(series, boundary, t):\n",
    "        import pickle\n",
    "        \n",
    "        count = Counter((series.progress_map(lambda x: x.split(\"|||\")).map(lambda x: x[2])))\n",
    "        \n",
    "        with open(f\"../dagobert-master/src/data/d_counter_{t}.p\", \"wb\") as fw:\n",
    "            pickle.dump(count, fw)\n",
    "        \n",
    "        \n",
    "        lst = []\n",
    "        for i in count.items():\n",
    "            for j in range(len(boundary)-1):\n",
    "                if boundary[j] <= i[1] < boundary[j+1]:\n",
    "                    lst.append(boundary[j])\n",
    "                    break\n",
    "\n",
    "        freq_bins = {i[0]: j for i, j in zip(count.items(), lst)}   \n",
    "        count_num = series.progress_map(lambda x: x.split(\"|||\")).map(lambda x: freq_bins[x[2]])\n",
    "        count_num.name = \"freq\"\n",
    "\n",
    "        return pd.concat([series.copy(), count_num.copy()], axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdeaee06",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_path = \"./data/cnn_dailymail_data/cnn_dailymail.csv\"\n",
    "prefix_path = \"../dagobert-master/data/external/prefix_list.txt\"\n",
    "semiprocessed_path = \"./data/cnn_dailymail_data/cnn_dailymail_semiprocessed.csv\"\n",
    "lemma_path = \"./data/cnn_dailymail_data/lemma_common.csv\"\n",
    "prefix_processed_path = \"./data/cnn_dailymail_data/prefix_preprocessed.csv\"\n",
    "binswhole_path = \"./data/cnn_dailymail_data/bins.txt\"\n",
    "binspartial_path = \"./data/cnn_dailymail_data/bins{:02d}.txt\"\n",
    "\n",
    "spacy_type = 'en_core_web_sm'\n",
    "\n",
    "def process(\n",
    "    original_path,\n",
    "    prefix_path,\n",
    "    semiprocessed_path,\n",
    "    lemma_path,\n",
    "    prefix_processed_path,\n",
    "    binswhole_path,  \n",
    "    binspartial_path,\n",
    "    spacy_type,\n",
    "    data_types,\n",
    "    num_cores = 1,\n",
    "    save_semiprocess = True,\n",
    "    save_lemma = True,\n",
    "    save_preprocess = True,\n",
    "    save_bins = True,\n",
    "    save_subbins = True,\n",
    "    slice_len = None\n",
    "):\n",
    "\n",
    "    prefix_list = open(IOUtils.recentFile(prefix_path), \"r\").read().split(\"\\n\")\n",
    "    semi_processor = Semiprocessing()\n",
    "    # processor = Semiprocessing(prefix_list, load_spacy = False)\n",
    "    \n",
    "    parallel = ParallelizingUtils(semi_processor.process_assign)\n",
    "    # Semiprocessing\n",
    "    print(\"saving semiprocessed files...\")    \n",
    "    if save_semiprocess:\n",
    "        \n",
    "        # read original data\n",
    "        cnn_path = IOUtils.recentFile(original_path)\n",
    "        if slice_len == None:\n",
    "            cnn_data = pd.read_csv(cnn_path, usecols = [\"article\"])\n",
    "        else:\n",
    "            cnn_data = pd.read_csv(cnn_path, usecols = [\"article\"], nrows = slice_len)\n",
    "       \n",
    "        # process original data: df: ['article': text] -> df: ['article': processed text]\n",
    "        cnn_data_semiprocessed = parallel.do_series(cnn_data.article, num_cores = num_cores, pre_assign = True)\n",
    "        filters = CheckUtils.checkSeries(cnn_data_semiprocessed,\n",
    "                                         isNan = True,\n",
    "                                         isEmpty = True,\n",
    "                                         isInf = False,\n",
    "                                         isNotInstance = str).map(lambda x: not x)\n",
    "        cnn_data_semiprocessed = cnn_data_semiprocessed[filters].drop_duplicates().reset_index(drop = True)        \n",
    "        cnn_data_semiprocessed = IOUtils.checkpoint_save(IOUtils.existsFile(semiprocessed_path), \n",
    "                                                         cnn_data_semiprocessed, \n",
    "                                                         data_type = \"dataFrame\", \n",
    "                                                         file_type = \"csv\", \n",
    "                                                         index_dataFrame = False)\n",
    "        print(\"done\")\n",
    "        \n",
    "    cnn_data_semiprocessed = IOUtils.checkpoint_load(IOUtils.recentFile(semiprocessed_path),\n",
    "                                                 data_type = \"dataFrame\",\n",
    "                                                 file_type = \"csv\")    \n",
    "\n",
    "    # load prefix_dic(successive hashmap dictionary)\n",
    "    prefix_structure = MakePrefixStructs(prefix_path)\n",
    "    lemmaMaker = LemmaMaker(prefix_structure.prefix_hash, spacy_type, load_spacy = True)\n",
    "    \n",
    "    if save_lemma: \n",
    "        # lemmatize\n",
    "        parallel.changeFunc(lemmaMaker.lemmatize_assign)\n",
    "        print(\"extracting frequent lemmas: lemmatizing\")\n",
    "        cnn_data_process = parallel.do_series(cnn_data_semiprocessed.article, num_cores = num_cores, pre_assign = True)\n",
    "        \n",
    "        filters = CheckUtils.checkSeries(cnn_data_process,                                  \n",
    "                                         isNan = True,                                 \n",
    "                                         isEmpty = True,                                 \n",
    "                                         isInf = False,                                 \n",
    "                                         isNotInstance = None,                                        \n",
    "                                         isNotIn = False,                                       \n",
    "                                         isNotInVal = None).map(lambda x: not x)\n",
    "        cnn_data_process = cnn_data_process[filters].reset_index(drop = True)\n",
    "        \n",
    "        \n",
    "        print(\"extracting frequent lemmas: removing prefixes\")\n",
    "        parallel.changeFunc(partial(lemmaMaker.find_lemma_wo_prefix, prefix_list = prefix_structure.prefix_list))\n",
    "        cnn_data_process = parallel.do_series(cnn_data_process, num_cores = num_cores, pre_assign = False)\n",
    "        cnn_data_process = pd.DataFrame(list(cnn_data_process))\n",
    "        filters = CheckUtils.checkSeries(cnn_data_process,                                  \n",
    "                                         isNan = True,                                 \n",
    "                                         isEmpty = True,                                 \n",
    "                                         isInf = False,                                 \n",
    "                                         isNotInstance = None,                                    \n",
    "                                         isNotIn = False,                                       \n",
    "                                         isNotInVal = None).map(lambda x: not x)\n",
    "        cnn_data_process = cnn_data_process[filters].reset_index(drop = True)\n",
    "        \n",
    "        print(\"done\")\n",
    "    # do not needed when processing reddit text, just load the lemma_common.csv\n",
    "        print(\"extracting frequent lemmas: extracting frequent lemma\")\n",
    "        \n",
    "        count = Counter((\" \".join(cnn_data_process.lemma)).split(\" \")).most_common(1000)\n",
    "        lemma_frequent = set([i[0] for i in count if len(i[0]) > 2 ])\n",
    "        \n",
    "        print(\"saving lemma...\")\n",
    "        lemma_frequent = pd.Series(sorted(list(lemma_frequent)))\n",
    "        lemma_frequent = IOUtils.checkpoint_save(IOUtils.existsFile(lemma_path), \n",
    "                                                 lemma_frequent, \n",
    "                                                 data_type = \"dataFrame\", \n",
    "                                                 file_type = \"csv\", \n",
    "                                                 index_dataFrame = False)\n",
    "        print(\"done\")\n",
    "    else:    \n",
    "        parallel.changeFunc(partial(lemmaMaker.find_prefix_with_existing_lemma, prefix_list = prefix_structure.prefix_list))\n",
    "        cnn_data_process = parallel.do_series(cnn_data_semiprocessed.article, num_cores = num_cores, pre_assign = False)\n",
    "        cnn_data_process = pd.DataFrame(list(cnn_data_process))\n",
    "        filters = CheckUtils.checkSeries(cnn_data_process,                                  \n",
    "                                         isNan = True,                                 \n",
    "                                         isEmpty = True,                                 \n",
    "                                         isInf = False,                                 \n",
    "                                         isNotInstance = str,\n",
    "                                         isNotIn = False,                                       \n",
    "                                         isNotInVal = None).map(lambda x: not x)\n",
    "        cnn_data_process = cnn_data_process[filters].reset_index(drop = True)\n",
    "\n",
    "        \n",
    "    lemma_frequent = IOUtils.checkpoint_load(IOUtils.recentFile(lemma_path),\n",
    "                                             data_type = \"dataFrame\",\n",
    "                                             file_type = \"csv\")\n",
    "    lemma_frequent = list(lemma_frequent['0'])\n",
    "    \n",
    "\n",
    "    if save_preprocess:\n",
    "        # check if the derivatives candidates are computationally derivatives\n",
    "        print(\"checking prefixes...\")\n",
    "        lemmaMaker.load_spacy_model(spacy_type)\n",
    "        binsMaker = BinsMaker(lemma_frequent= lemma_frequent, prefix_list = prefix_structure.prefix_list, load_model = lemmaMaker.load_model)\n",
    "        parallel.changeFunc(binsMaker.isRealPrefix)\n",
    "        cnn_data_process = parallel.do_series(cnn_data_process.prefixed_tokens, num_cores = num_cores, pre_assign = False)   \n",
    "        cnn_data_process = BinsMaker.postprocess(cnn_data_process)\n",
    "        \n",
    "        filters = CheckUtils.checkSeries(cnn_data_process,                                  \n",
    "                                         isNan = True,                                                                      \n",
    "                                         isEmpty = True,                                                                     \n",
    "                                         isInf = False,                                                                      \n",
    "                                         isNotInstance = None,\n",
    "                                         isNotIn = False,                                       \n",
    "                                         isNotInVal = None).map(lambda x: not x)\n",
    "        cnn_data_process = cnn_data_process[filters].reset_index(drop = True)\n",
    "        cnn_data_process = IOUtils.checkpoint_save(IOUtils.existsFile(prefix_processed_path), \n",
    "                                                   cnn_data_process, \n",
    "                                                   data_type = \"dataFrame\", \n",
    "                                                   file_type = \"csv\",                                          \n",
    "                                                   index_dataFrame = False)\n",
    "        print(\"done\")\n",
    "\n",
    "    \n",
    "    cnn_data_process = IOUtils.checkpoint_load(IOUtils.recentFile(prefix_processed_path),\n",
    "                                                 data_type = \"dataFrame\",\n",
    "                                                 file_type = \"csv\")    \n",
    "\n",
    "    if save_bins:\n",
    "        print(\"binning...\")\n",
    "        if slice_len == None:\n",
    "            bins = pd.read_csv(IOUtils.recentFile(semiprocessed_path), usecols = [\"article\"])\n",
    "        else:\n",
    "            bins = pd.read_csv(IOUtils.recentFile(semiprocessed_path), usecols = [\"article\"], nrows = slice_len)\n",
    "\n",
    "        \n",
    "        \n",
    "        bins = BinsMaker.word_to_blank(cnn_data_process.copy(), bins.article.copy())     \n",
    "        bins = BinsMaker.split_sentence(bins.copy())\n",
    "    \n",
    "        filters = CheckUtils.checkSeries(bins,\n",
    "                                     isNan = True,\n",
    "                                     isEmpty = True,\n",
    "                                     isInf = False,\n",
    "                                     isNotInstance = str,                                         \n",
    "                                         isNotIn = False,                                       \n",
    "                                         isNotInVal = None).map(lambda x: not x)\n",
    "        bins = bins[filters].drop_duplicates().reset_index(drop = True)\n",
    "        bins = bins.explode().reset_index(drop = True)\n",
    "        \n",
    "        bins = IOUtils.checkpoint_save(IOUtils.existsFile(binswhole_path), \n",
    "                                                   bins, \n",
    "                                                   data_type = \"dataFrame\", \n",
    "                                                   file_type = \"csv\",                                          \n",
    "                                                   index_dataFrame = False)\n",
    "\n",
    "        \n",
    "        print(\"done\")\n",
    "    \n",
    "    if save_subbins:\n",
    "        print(\"subbinning...\")\n",
    "        bins = IOUtils.checkpoint_load(IOUtils.recentFile(binswhole_path),\n",
    "                                                     data_type = \"dataFrame\",\n",
    "                                                     file_type = \"csv\")  \n",
    "        boundary = [1,2,4,8,16,32,64,128,100000000 ]\n",
    "        bins = bins[bins.article.map(lambda x: isinstance(x, str))].reset_index(drop = True)\n",
    "        bins = BinsMaker.classify_bins(bins.article, boundary, data_types)\n",
    "\n",
    "        ser = bins.groupby('freq')['article'].apply(list)\n",
    "        for freq in boundary[:-1]:\n",
    "            if freq in ser.index:\n",
    "                pd.DataFrame(ser[freq]).to_csv(IOUtils.existsFile(binspartial_path.format(freq)), header = False, index = False)\n",
    "        print(\"done\")\n",
    "        \n",
    "    print(\"Whole Jobs Done. Good Job\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bf58bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time \n",
    "\n",
    "# original_path = \"./data/cnn_dailymail_data/cnn_dailymail.csv\"\n",
    "# prefix_path = \"../dagobert-master/data/external/prefix_list.txt\"\n",
    "# semiprocessed_path_temp = \"./data/cnn_dailymail_data/temp/cnn_dailymail_semiprocessed.csv\"\n",
    "# lemma_path_temp = \"./data/cnn_dailymail_data/temp/lemma_common.csv\"\n",
    "# prefix_processed_path_temp = \"./data/cnn_dailymail_data/temp/prefix_preprocessed.csv\"\n",
    "# binswhole_path_temp = \"./data/cnn_dailymail_data/temp/bins.txt\"\n",
    "# binspartial_path_temp = \"./data/cnn_dailymail_data/temp/bins{:02d}.txt\"\n",
    "# slice_len = 1000\n",
    "# spacy_type = 'en_core_web_sm'\n",
    "\n",
    "# x = process(original_path = original_path,\n",
    "#        prefix_path = prefix_path,\n",
    "#        semiprocessed_path = semiprocessed_path_temp,\n",
    "#        lemma_path = lemma_path_temp,\n",
    "#        prefix_processed_path = prefix_processed_path_temp,\n",
    "#        binswhole_path = binswhole_path_temp,\n",
    "#        binspartial_path = binspartial_path_temp,\n",
    "#         spacy_type = spacy_type,\n",
    "#         num_cores = 32,\n",
    "#         save_lemma = True,\n",
    "#         slice_len = slice_len\n",
    "#        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f9ce1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original_path = \"../dagobert-master/data/final/raw.txt\"\n",
    "# partial_bin_path = \"../dagobert-master/data/final/bin{:02d}.txt\"\n",
    "\n",
    "# def reverse_process(partial_bin_path, output_path, boundary = [1,2,4,8,16,32,64]):    \n",
    "#     x = pd.DataFrame()\n",
    "#     for b in boundary:\n",
    "#         print(\"merging {:02d}th bin\".format(b))\n",
    "#         bin01 = [i.strip() for i in open(IOUtils.recentFile(partial_bin_path.format(b)), \"r\").readlines()]\n",
    "#         bin01 = pd.DataFrame([i.split(\"|||\") for i in bin01])\n",
    "#         bin01 = bin01.apply(lambda x: x[3] + re.sub(\"___\", x[1], x[4]) + x[5], axis = 1)    \n",
    "#         bin01.name = \"article\"\n",
    "#         bin01 = bin01.to_frame()\n",
    "#         x = pd.concat([x, bin01], axis = 0)\n",
    "#     return x\n",
    "\n",
    "# bin01 = reverse_process(partial_bin_path, original_path)ㄴ\n",
    "# bin01.to_csv(IOUtils.existsFile(original_path), index = False)\n",
    "# bin01 = pd.read_csv(IOUtils.recentFile(original_path))\n",
    "# bin01 = bin01.drop_duplicates().reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bff9bf40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '', '']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#process with bert \n",
    "\n",
    "\"||||||\".split(\"|||\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04458a75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# original_path = \"./data/cnn_dailymail_data/cnn_dailymail.csv\"\n",
    "# prefix_path = \"../dagobert-master/data/external/prefix_list.txt\"\n",
    "# semiprocessed_path_temp = \"./data/cnn_dailymail_data/cnn_dailymail_semiprocessed.csv\"\n",
    "# lemma_path_temp = \"./data/cnn_dailymail_data/lemma_common.csv\"\n",
    "# prefix_processed_path_temp = \"./data/cnn_dailymail_data/prefix_preprocessed.csv\"\n",
    "# binswhole_path_temp = \"./data/cnn_dailymail_data/bins.txt\"\n",
    "# binspartial_path_temp = \"./data/cnn_dailymail_data/temp/temp/bins{:02d}.txt\"\n",
    "# spacy_type = 'en_core_web_sm'\n",
    "\n",
    "# slice_len = None\n",
    "# spacy_type = 'en_core_web_sm'\n",
    "\n",
    "# x = process(original_path = original_path,\n",
    "#        prefix_path = prefix_path,\n",
    "#        semiprocessed_path = semiprocessed_path_temp,\n",
    "#        lemma_path = lemma_path_temp,\n",
    "#        prefix_processed_path = prefix_processed_path_temp,\n",
    "#        binswhole_path = binswhole_path_temp,\n",
    "#        binspartial_path = binspartial_path_temp,\n",
    "#             data_types = \"cnn\",\n",
    "#         spacy_type = spacy_type,\n",
    "#         num_cores = 40,\n",
    "#         save_semiprocess = False,\n",
    "#         save_lemma = False,         \n",
    "#         save_preprocess = False,    \n",
    "#         save_bins = True,    \n",
    "#         save_subbins = True,\n",
    "#         slice_len = slice_len\n",
    "#        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6dc3a968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9eb51c18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The url of the fIle to be loaded is /home/hyohyeongjang/DagoBERT/dagobert-master/data/external/prefix_list.txt\n",
      "saving semiprocessed files...\n",
      "The url of the fIle to be loaded is /home/hyohyeongjang/DagoBERT/dagobert-master/data/raw_semiprocessed.csv\n",
      "The url of the fIle to be loaded is /home/hyohyeongjang/DagoBERT/dagobert-master/data/raw_semiprocessed.csv\n",
      "The url of the fIle to be loaded is /home/hyohyeongjang/DagoBERT/dagobert-master/data/external/prefix_list.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hyohyeongjang/.conda/envs/hyohyeongjang_base/lib/python3.9/site-packages/torch/cuda/__init__.py:628: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/hyohyeongjang/.conda/envs/hyohyeongjang_base/lib/python3.9/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'Series.swapaxes' is deprecated and will be removed in a future version. Please use 'Series.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 65673/65673 [00:02<00:00, 23323.35it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 65673/65673 [00:02<00:00, 23681.69it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 65673/65673 [00:02<00:00, 23701.32it/s]\n",
      " 79%|██████████████████████████████████████████████████████████▌               | 51981/65673 [00:02<00:00, 25148.28it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 65673/65673 [00:02<00:00, 23459.95it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 65673/65673 [00:03<00:00, 21539.55it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 65673/65673 [00:02<00:00, 23525.49it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 65673/65673 [00:02<00:00, 23471.51it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 65673/65673 [00:02<00:00, 22170.58it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 65673/65673 [00:02<00:00, 23257.41it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 65673/65673 [00:02<00:00, 22947.17it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 65673/65673 [00:03<00:00, 21889.86it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 65673/65673 [00:02<00:00, 23691.93it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 65673/65673 [00:02<00:00, 23602.59it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 65673/65673 [00:02<00:00, 23460.40it/s]\n",
      " 43%|████████████████████████████████▏                                         | 28526/65673 [00:01<00:01, 25210.64it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 65673/65673 [00:02<00:00, 23460.23it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 65673/65673 [00:02<00:00, 23185.15it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 65673/65673 [00:02<00:00, 22871.73it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 65673/65673 [00:02<00:00, 23054.68it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 65673/65673 [00:02<00:00, 23278.65it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 65673/65673 [00:02<00:00, 23772.29it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 65673/65673 [00:02<00:00, 23297.37it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 65673/65673 [00:02<00:00, 23137.40it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 65672/65672 [00:02<00:00, 23644.33it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 65672/65672 [00:02<00:00, 23458.86it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 65672/65672 [00:02<00:00, 23244.51it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 65672/65672 [00:02<00:00, 23491.66it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 65672/65672 [00:02<00:00, 23384.51it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 65672/65672 [00:02<00:00, 23567.82it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 65672/65672 [00:02<00:00, 23382.97it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 65672/65672 [00:02<00:00, 24044.79it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 65672/65672 [00:02<00:00, 23805.32it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 65672/65672 [00:02<00:00, 23473.83it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 65672/65672 [00:02<00:00, 23590.13it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 65672/65672 [00:02<00:00, 23368.47it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 65672/65672 [00:02<00:00, 23331.74it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 65672/65672 [00:02<00:00, 23822.71it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 65672/65672 [00:02<00:00, 23101.29it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 65672/65672 [00:02<00:00, 23460.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The url of the fIle to be loaded is /home/hyohyeongjang/DagoBERT/dagobert-master/data/lemma_common.csv\n",
      "The url of the fIle to be loaded is /home/hyohyeongjang/DagoBERT/dagobert-master/data/lemma_common.csv\n",
      "checking prefixes...\n",
      "spacy model is already loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hyohyeongjang/.conda/envs/hyohyeongjang_base/lib/python3.9/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'Series.swapaxes' is deprecated and will be removed in a future version. Please use 'Series.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot concatenate unaligned mixed dimensional NDFrame objects",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m slice_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     10\u001b[0m spacy_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 12\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moriginal_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m       \u001b[49m\u001b[43mprefix_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mprefix_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m       \u001b[49m\u001b[43msemiprocessed_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msemiprocessed_path_temp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m       \u001b[49m\u001b[43mlemma_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlemma_path_temp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m       \u001b[49m\u001b[43mprefix_processed_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mprefix_processed_path_temp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m       \u001b[49m\u001b[43mbinswhole_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbinswhole_path_temp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m       \u001b[49m\u001b[43mbinspartial_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbinspartial_path_temp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mspacy_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mspacy_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata_types\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreddit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_cores\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_semiprocess\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_lemma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m         \u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_preprocess\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_bins\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_subbins\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mslice_len\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mslice_len\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m       \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 140\u001b[0m, in \u001b[0;36mprocess\u001b[0;34m(original_path, prefix_path, semiprocessed_path, lemma_path, prefix_processed_path, binswhole_path, binspartial_path, spacy_type, data_types, num_cores, save_semiprocess, save_lemma, save_preprocess, save_bins, save_subbins, slice_len)\u001b[0m\n\u001b[1;32m    138\u001b[0m parallel\u001b[38;5;241m.\u001b[39mchangeFunc(binsMaker\u001b[38;5;241m.\u001b[39misRealPrefix)\n\u001b[1;32m    139\u001b[0m cnn_data_process \u001b[38;5;241m=\u001b[39m parallel\u001b[38;5;241m.\u001b[39mdo_series(cnn_data_process\u001b[38;5;241m.\u001b[39mprefixed_tokens, num_cores \u001b[38;5;241m=\u001b[39m num_cores, pre_assign \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)   \n\u001b[0;32m--> 140\u001b[0m cnn_data_process \u001b[38;5;241m=\u001b[39m \u001b[43mBinsMaker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpostprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcnn_data_process\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m filters \u001b[38;5;241m=\u001b[39m CheckUtils\u001b[38;5;241m.\u001b[39mcheckSeries(cnn_data_process,                                  \n\u001b[1;32m    143\u001b[0m                                  isNan \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,                                                                      \n\u001b[1;32m    144\u001b[0m                                  isEmpty \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,                                                                     \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    147\u001b[0m                                  isNotIn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,                                       \n\u001b[1;32m    148\u001b[0m                                  isNotInVal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m)\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;129;01mnot\u001b[39;00m x)\n\u001b[1;32m    149\u001b[0m cnn_data_process \u001b[38;5;241m=\u001b[39m cnn_data_process[filters]\u001b[38;5;241m.\u001b[39mreset_index(drop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[5], line 36\u001b[0m, in \u001b[0;36mBinsMaker.postprocess\u001b[0;34m(series)\u001b[0m\n\u001b[1;32m     34\u001b[0m x \u001b[38;5;241m=\u001b[39m series\u001b[38;5;241m.\u001b[39mexplode()\n\u001b[1;32m     35\u001b[0m x \u001b[38;5;241m=\u001b[39m x[x\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mtuple\u001b[39m))]\n\u001b[0;32m---> 36\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSeries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43midx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_records\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtoken\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprefix\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlemma\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/.conda/envs/hyohyeongjang_base/lib/python3.9/site-packages/pandas/core/reshape/concat.py:382\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 382\u001b[0m op \u001b[38;5;241m=\u001b[39m \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[0;32m~/.conda/envs/hyohyeongjang_base/lib/python3.9/site-packages/pandas/core/reshape/concat.py:469\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;66;03m# if we have mixed ndims, then convert to highest ndim\u001b[39;00m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;66;03m# creating column numbers as needed\u001b[39;00m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ndims) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 469\u001b[0m     objs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sanitize_mixed_ndim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjs \u001b[38;5;241m=\u001b[39m objs\n\u001b[1;32m    473\u001b[0m \u001b[38;5;66;03m# note: this is the BlockManager axis (since DataFrame is transposed)\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/hyohyeongjang_base/lib/python3.9/site-packages/pandas/core/reshape/concat.py:599\u001b[0m, in \u001b[0;36m_Concatenator._sanitize_mixed_ndim\u001b[0;34m(self, objs, sample, ignore_index, axis)\u001b[0m\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m ndim \u001b[38;5;241m!=\u001b[39m max_ndim \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 599\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    600\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot concatenate unaligned mixed dimensional NDFrame objects\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    601\u001b[0m     )\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    604\u001b[0m     name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mValueError\u001b[0m: cannot concatenate unaligned mixed dimensional NDFrame objects"
     ]
    }
   ],
   "source": [
    "original_path = \"../dagobert-master/data/raw.txt\"\n",
    "prefix_path = \"../dagobert-master/data/external/prefix_list.txt\"\n",
    "semiprocessed_path_temp = \"../dagobert-master/data/raw_semiprocessed.csv\"\n",
    "lemma_path_temp = \"../dagobert-master/data/lemma_common.csv\"\n",
    "prefix_processed_path_temp = \"../dagobert-master/data/raw_prefix_processed.csv\"\n",
    "binswhole_path_temp = \"../dagobert-master/data/raw_bins.csv\"\n",
    "binspartial_path_temp = \"../dagobert-master/data/temp/raw_bins{:02d}.txt\"\n",
    "\n",
    "slice_len = None\n",
    "spacy_type = 'en_core_web_sm'\n",
    "\n",
    "x = process(original_path = original_path,\n",
    "       prefix_path = prefix_path,\n",
    "       semiprocessed_path = semiprocessed_path_temp,\n",
    "       lemma_path = lemma_path_temp,\n",
    "       prefix_processed_path = prefix_processed_path_temp,\n",
    "       binswhole_path = binswhole_path_temp,\n",
    "       binspartial_path = binspartial_path_temp,\n",
    "        spacy_type = spacy_type,\n",
    "            data_types = \"reddit\",\n",
    "        num_cores = 40,\n",
    "        save_semiprocess = True,\n",
    "        save_lemma = True,         \n",
    "        save_preprocess = True,    \n",
    "        save_bins = True,    \n",
    "        save_subbins = True,\n",
    "        slice_len = slice_len\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f60183",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2178ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f690029",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9a44fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d8bbc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2d27a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15184133",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980c161c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca74a63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c023e917",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109a9619",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1553d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0ac524",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e2f768",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8a0910",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12065b82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a892e5bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b002fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56bc8a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# out = cnn_data_process.map(lambda x: list(filter(lambda y: y[0] != \"-\", x)))\n",
    "\n",
    "# x = out.explode()\n",
    "# x = x[x.map(lambda x: isinstance(x, tuple))]\n",
    "\n",
    "# out = pd.concat([pd.Series(x.index, name = \"idx\"), pd.DataFrame.from_records(x.values, columns = [\"token\", \"prefix\", \"lemma\"])], axis = 1)\n",
    "# out_url = existsFile(\"./data/cnn_dailymail_data/prefix_preprocessed.csv\")\n",
    "# out.to_csv(out_url, encoding = \"utf-8\", index = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb50cc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ppr_url = Path(\"./data/cnn_dailymail_data/prefix_preprocessed.csv\").resolve()\n",
    "# lem_url = Path(\"./data/cnn_dailymail_data/lemma_common.csv\").resolve()\n",
    "# cnn_url = Path(\"./data/cnn_dailymail_data/cnn_dailymail_semiprocessed.csv\").resolve()\n",
    "\n",
    "# cnn_pre = pd.read_csv(ppr_url)\n",
    "# lemma = pd.read_csv(lem_url)\n",
    "# cnn_data = pd.read_csv(cnn_url, usecols = ['article'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417470ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
