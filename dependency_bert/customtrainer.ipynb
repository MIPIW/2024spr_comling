{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f660868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t, e, v = load_dataset()\n",
    "\n",
    "# train_dataset = load_data_with_custom_mask(dataset = t, slice_len = 100)\n",
    "# eval_dataset = load_data_with_custom_mask(dataset = e, slice_len = 100)\n",
    "\n",
    "\n",
    "# from transformers import AutoModelForMaskedLM, AutoConfig, AutoTokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "# model = AutoModelForMaskedLM.from_config(config = AutoConfig.from_pretrained(\"google-bert/bert-base-uncased\"))\n",
    "\n",
    "# train_dataset = DatasetWithCustomMasking(train_dataset, tokenizer,                          \n",
    "#                                          data_args = 'preprocessed',                          \n",
    "#                                          mask_args = \"encoder_attention_mask\",                       \n",
    "#                                          padding = 'max_length',\n",
    "#                                          max_length = 128,\n",
    "#                                          return_tensors = \"pt\")\n",
    "# eval_dataset = DatasetWithCustomMasking(eval_dataset, tokenizer, \n",
    "#                                         data_args = 'preprocessed',                                                   \n",
    "#                                         mask_args = \"encoder_attention_mask\",                                                \n",
    "#                                         padding = 'max_length',                         \n",
    "#                                         max_length = 128,                         \n",
    "#                                         return_tensors = \"pt\")\n",
    "# from transformers import DataCollatorForLanguageModeling\n",
    "# data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, \n",
    "#                                                 mlm=True, \n",
    "#                                                 mlm_probability=0.15)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "103b928b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "# def train(*,\n",
    "#           model,\n",
    "#           tokenizer,\n",
    "#           train_dataset,\n",
    "#           eval_dataset,\n",
    "#           do_eval = True,\n",
    "#           do_save = True,\n",
    "#           eval_step,\n",
    "#           save_step,\n",
    "#           save_dir,\n",
    "#           save_at_most = 2,\n",
    "#           save_by,\n",
    "#           num_epoch,         \n",
    "#           batch_size,\n",
    "#           data_collator,          \n",
    "#           dataloader_config = None,        \n",
    "#           tokenizer_config = None,\n",
    "#           optimizer_config = None,\n",
    "#           loss_config = None,\n",
    "#           seed = 42,\n",
    "#           evaluation,\n",
    "#           metric,\n",
    "#          ):\n",
    "    \n",
    "#     # optimizer = optim.Adam(params = model.parameters(), lr = LEARNING_RATE, weight_decay = WEIGHT_DECAY, eps = ADAM_EPS)\n",
    "#     # criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "#     torch.manual_seed(seed)\n",
    "#     np.random.seed(seed)\n",
    "#     torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "#     from torch import optim, nn\n",
    "#     lr = optimizer_config['lr']\n",
    "#     weight_decay = optimizer_config['weight_decay']\n",
    "#     eps = optimizer_config['eps']\n",
    "    \n",
    "#     if optimizer_config['type'] == 'adam':\n",
    "#         optimizer = optim.Adam(params = model.parameters(), lr = lr, weight_decay = weight_decay, eps = eps)\n",
    "# #     if criterion_config['type'] == \"crossEntropyLoss\":\n",
    "# #         criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "#     from torch.utils.data import DataLoader\n",
    "    \n",
    "#     shuffle = dataloader_config.pop('shuffle', None)\n",
    "#     batch_size = dataloader_config.pop('batch_size', None)\n",
    "    \n",
    "#     train_dataloader = DataLoader(train_dataset, batch_size = 16, shuffle = shuffle)\n",
    "#     eval_dataloader = DataLoader(eval_dataset, batch_size = 16, shuffle = shuffle)\n",
    "    \n",
    "#     cuda = \"cuda\" if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "#     model.train()\n",
    "    \n",
    "#     from pathlib import Path\n",
    "#     if save_by == 'accuracy':\n",
    "#         max_args = [(0,0)] * save_at_most\n",
    "#     if save_by == 'loss':\n",
    "#         max_args = [(0,10000)] * save_at_most\n",
    "#     saved_step = [None, None]\n",
    "    \n",
    "#     step = 0\n",
    "#     for epoch in range(num_epoch):\n",
    "        \n",
    "#         for data in train_dataloader:\n",
    "#             masked = data_collator([data])\n",
    "#             masked = {i: masked[i].squeeze(0) for i in masked}\n",
    "# #             masked['attention_mask'] = masked['encoder_attention_mask']\n",
    "# #             masked.pop('encoder_attention_mask')\n",
    "\n",
    "#             for j in masked:\n",
    "#                 masked[j] = masked[j].to(cuda)\n",
    "#             model = model.to(cuda)\n",
    "            \n",
    "#             optimizer.zero_grad() # 이렇게만 해도 되나? loss.backward() 이거 없이?\n",
    "#             out = model(**masked)\n",
    "#             optimizer.step()\n",
    "#             step += 1\n",
    "        \n",
    "#             if step % eval_step == 0:\n",
    "#                 eval_loss, eval_accuracy = evaluation(model = model,                           \n",
    "#                        eval_dataloader = eval_dataloader, \n",
    "#                        metric = metric,\n",
    "#                        data_collator = data_collator,\n",
    "#                        step = step)\n",
    "                \n",
    "#                 if save_by == 'accuracy':\n",
    "#                     the_value = eval_accuracy\n",
    "                    \n",
    "#                     if min(max_args) < the_value:\n",
    "                        \n",
    "#                         print(f\"scored {save_by}: {the_value} among {', '.join([str(i) for i in max_args])}\")\n",
    "#                         idx = max_args.index(min(max_args, key = lambda x: x[1]))\n",
    "#                         max_args[idx] = (step, round(the_value.item(),2))\n",
    "\n",
    "#                         save_dir_temp = Path(save_dir + os.sep + f\"model_{step}.pt\").resolve()\n",
    "#                         save_dir_temp = IOUtils.existsFile(save_dir_temp)\n",
    "#                         torch.save(model, save_dir_temp)\n",
    "#                         #### 두 개 이상일 경우에는 하나 제거\n",
    "               \n",
    "#                 elif save_by == \"loss\":\n",
    "#                     the_value = eval_loss\n",
    "                    \n",
    "#                     if max(max_args) > the_value:\n",
    "#                         print(f\"scored {save_by}: {the_value} among {', '.join([str(i) for i in max_args])}\")\n",
    "#                         idx = max_args.index(max(max_args, key = lambda x: x[1]))\n",
    "#                         max_args[idx] = (step, round(the_value.item(),2))\n",
    "\n",
    "#                         save_dir_temp = Path(save_dir + os.sep + f\"model_{step}.pt\").resolve()\n",
    "#                         save_dir_temp = IOUtils.existsFile(save_dir_temp)\n",
    "#                         torch.save(model, save_dir_temp)\n",
    "#                 else:\n",
    "#                     print(\"not to be saved\")\n",
    "#                     the_value = -1\n",
    "                \n",
    "#                 print(the_value)\n",
    "                \n",
    "                \n",
    "                                   \n",
    "\n",
    "#                       #safetensor로 어떻게 저장히지?##\n",
    "            \n",
    "# #                     safetensors.torch.save_file(model, save_dir)\n",
    "# #                     config.json\n",
    "# #                     tokenizer_config.json\n",
    "# #                     trainer_state.json\n",
    "# #                     vocab.txt\n",
    "# #                     generation_config.json\n",
    "# #                     special_tokens_map.json\n",
    "# #                     tokenizer.json\n",
    "# #                     training_args.bin\n",
    "\n",
    "\n",
    "                \n",
    "#     return train_dataloader, data_collator\n",
    "    \n",
    "#     return train_dataset, data_collator\n",
    "\n",
    "\n",
    "# def metric(predictions, labels):\n",
    "#     import evaluate\n",
    "#     accuracy = evaluate.load('accuracy')\n",
    "\n",
    "#     filters = labels != -100\n",
    "    \n",
    "#     predictions = predictions[filters]\n",
    "#     labels = labels[filters]\n",
    "    \n",
    "#     return accuracy.compute(predictions = predictions, references = labels)\n",
    "\n",
    "# def evaluate(*,              \n",
    "#              model,             \n",
    "#              eval_dataloader,                           \n",
    "#              data_collator,\n",
    "#              metric,\n",
    "#              step,\n",
    "#              seed = 42,\n",
    "             \n",
    "#               ):\n",
    "#     import torch\n",
    "#     import safetensors\n",
    "#     import os\n",
    "    \n",
    "#     cuda = \"cuda\" if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    \n",
    "#     total_loss = 0\n",
    "#     total_logits = None\n",
    "#     total_labels = None\n",
    "    \n",
    "#     torch.manual_seed(seed)\n",
    "#     np.random.seed(seed)\n",
    "#     torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         for i, data in enumerate(eval_dataloader):\n",
    "#             masked = data_collator([data])\n",
    "#             masked = {i: masked[i].squeeze(0) for i in masked}\n",
    "# #             masked['attention_mask'] = masked['encoder_attention_mask']\n",
    "# #             masked.pop('encoder_attention_mask')\n",
    "            \n",
    "#             for j in masked:\n",
    "#                 masked[j] = masked[j].to(cuda)\n",
    "#             model = model.to(cuda)\n",
    "            \n",
    "#             out = model(**masked)\n",
    "#             loss = out.loss\n",
    "#             logits = out.logits\n",
    "#             total_loss = total_loss + loss\n",
    "\n",
    "\n",
    "#             if total_logits == None:\n",
    "#                 total_logits = torch.argmax(logits, dim = -1).view(-1)\n",
    "#             else:\n",
    "#                 total_logits = torch.cat([total_logits, torch.argmax(logits, dim = -1).view(-1)]).contiguous()\n",
    "\n",
    "#             if total_labels == None:\n",
    "#                 total_labels = masked['labels'].view(-1).contiguous()\n",
    "#             else:\n",
    "#                 total_labels = torch.cat([total_labels, masked['labels'].view(-1)]).contiguous()\n",
    "\n",
    "#         accuracy = metric(total_logits, total_labels)['accuracy']\n",
    "    \n",
    "#     return total_loss, accuracy\n",
    "        \n",
    "\n",
    "        \n",
    "# masked = train(model = model,\n",
    "#      tokenizer = tokenizer,\n",
    "#      train_dataset = train_dataset,\n",
    "#      eval_dataset = eval_dataset,\n",
    "#       do_eval = True,\n",
    "#       do_save = True,\n",
    "#       eval_step = 2,\n",
    "#       save_step = 2,\n",
    "#       save_dir = \"./checkpoint_output\",\n",
    "#       save_by = 'loss',\n",
    "#       save_at_most = 2,\n",
    "#       num_epoch = 3,\n",
    "#       batch_size = 16,\n",
    "#       data_collator = data_collator,\n",
    "#       loss_config = loss_config, # autmodel에 다 들어있는 것 같은데?\n",
    "#       optimizer_config = optimizer_config,                                        \n",
    "#       dataloader_config = dataloader_config,\n",
    "#       tokenizer_config = tokenizer_config,\n",
    "#       evaluation = evaluate,\n",
    "#       metric = metric\n",
    "               \n",
    "#      )\n",
    "\n",
    "\n",
    "\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a71635",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
