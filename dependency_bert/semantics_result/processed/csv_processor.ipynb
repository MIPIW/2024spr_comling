{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math \n",
    "\n",
    "\n",
    "\n",
    "class CheckUtils():\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "    \n",
    "    # def checkValue(func):\n",
    "    #     from functools import partial\n",
    "    #     partial \n",
    "    @staticmethod\n",
    "    def checkSeries(serdf, \n",
    "                 isNan = True, \n",
    "                 isEmpty = True, \n",
    "                 isInf = True,\n",
    "                    isNotInstance = None,\n",
    "                    isNotIn = False,\n",
    "                    isNotInVal = None,\n",
    "                    types = None\n",
    "                 ):\n",
    "        \n",
    "\n",
    "        \n",
    "        def _check_subsequent(x, \n",
    "                              isNotInstance = isNotInstance, \n",
    "                              isNan = isNan, \n",
    "                              isEmpty = isEmpty, \n",
    "                              isInf = isInf, \n",
    "                              isnotIn = isNotIn, \n",
    "                              isNotInVal = isNotInVal):\n",
    "            out = False\n",
    "            \n",
    "            if isNotInstance is not None:\n",
    "                x1 = (not isinstance(x, isNotInstance))\n",
    "            else:\n",
    "                x1 = False\n",
    "\n",
    "            if isinstance(x, (int, float, complex)):\n",
    "                x2 = pd.isna(x) if isNan else False\n",
    "                x3 = math.isinf(x) if isInf else False\n",
    "                x4 = False\n",
    "            \n",
    "            elif x == None:\n",
    "                x2 = False\n",
    "                x3 = False\n",
    "                x4 = True if isEmpty else False\n",
    "            \n",
    "            else:\n",
    "                x2 = False\n",
    "                x3 = False\n",
    "                x4 = (len(x) == 0) if isEmpty else False\n",
    "                \n",
    "            x5 = isNotInVal not in x if isNotIn else False\n",
    "\n",
    "            return out | x1 | x2 | x3 | x4 | x5\n",
    "        \n",
    "        def postprocess(df, types):\n",
    "            if types == \"row\":\n",
    "                return df.map(lambda x: not x)\n",
    "            elif types == \"col\":\n",
    "                return df[df == False].index # columns\n",
    "            \n",
    "        if isinstance(serdf, pd.Series):\n",
    "            return serdf.progress_map(lambda x: _check_subsequent(x))\n",
    "        \n",
    "        elif isinstance(serdf, pd.DataFrame):\n",
    "            out  = serdf.map(lambda x: _check_subsequent(x)) # cellwise\n",
    "            if types == \"row\":\n",
    "                return  postprocess(out.apply(lambda x: any(x), axis = 1), \"row\")\n",
    "            elif types == \"column\":\n",
    "                return postprocess(out.apply(lambda x: any(x), axis = 0), \"col\")\n",
    "            else:\n",
    "                raise ValueError(\"check type is not specified. select among 'row' and 'column\")\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"this is not either series or dataframe.\")\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def isEmpty(dataframe) -> bool:\n",
    "    \n",
    "        if len(dataframe.index) == 0:\n",
    "            return True\n",
    "    \n",
    "        return False\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 done\n",
      "1 done\n",
      "2 done\n",
      "3 done\n",
      "4 done\n",
      "5 done\n",
      "6 done\n",
      "7 done\n",
      "8 done\n",
      "9 done\n",
      "10 done\n",
      "11 done\n",
      "12 done\n",
      "13 done\n",
      "14 done\n",
      "15 done\n",
      "16 done\n",
      "17 done\n",
      "18 done\n",
      "19 done\n",
      "20 done\n",
      "21 done\n",
      "22 done\n",
      "23 done\n",
      "24 done\n",
      "25 done\n",
      "26 done\n",
      "27 done\n",
      "28 done\n",
      "29 done\n",
      "30 done\n",
      "31 done\n",
      "0 done\n",
      "1 done\n",
      "2 done\n",
      "3 done\n",
      "4 done\n",
      "5 done\n",
      "6 done\n",
      "7 done\n",
      "8 done\n",
      "9 done\n",
      "10 done\n",
      "11 done\n",
      "12 done\n",
      "13 done\n",
      "14 done\n",
      "15 done\n",
      "16 done\n",
      "17 done\n",
      "18 done\n",
      "19 done\n",
      "20 done\n",
      "21 done\n",
      "22 done\n",
      "23 done\n",
      "24 done\n",
      "25 done\n",
      "26 done\n",
      "27 done\n",
      "28 done\n",
      "29 done\n",
      "30 done\n",
      "31 done\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import MSELoss\n",
    "import torch\n",
    "\n",
    "class Processor():\n",
    "    def __init__(self, base, url, types):\n",
    "        \n",
    "        raw_url = \"/home/hyohyeongjang/dependency_bert/semantics_result/raw/\"\n",
    "        dir = os.listdir(raw_url)\n",
    "        dir = [i for i in dir if i.split(\".\")[0].split(\"_\")[-1] == types]\n",
    "\n",
    "        model_types, attention_types = ['dependency', \"normal\"], [\"front\", 'back', \"none\"]\n",
    "\n",
    "        out = [[],[]]\n",
    "        for i in dir:\n",
    "            for model_index, model_type in enumerate(model_types):\n",
    "\n",
    "                for attention_index, attention_type in enumerate(attention_types):\n",
    "                    if (attention_type == i.split(\"_\")[2]) and (model_type in i.split(\"_\")[1]):\n",
    "                        out[model_index].append(raw_url+i)\n",
    "\n",
    "        self.out = out\n",
    "\n",
    "        out_df = []            \n",
    "        for merges in out:\n",
    "            df = pd.DataFrame()\n",
    "            for idx, file in enumerate(merges):\n",
    "                df_temp = pd.read_csv(file).drop(columns = \"Unnamed: 0\")\n",
    "                if idx == 0:\n",
    "                    df = df_temp\n",
    "                else:\n",
    "                    df = df.merge(df_temp, on = ['type', 'quant', 'frontness', \"comple\"])\n",
    "\n",
    "            out_df.append(df)\n",
    "            \n",
    "        df = pd.concat(out_df, axis = 0).reset_index(drop = True)\n",
    "        \n",
    "\n",
    "        self.col_front = [f\"f{i}\" for i in range(1,10)] \n",
    "        self.col_back = [f\"b{i}\" for i in range(1,10)]\n",
    "        self.col_full = [f\"n{i}\" for i in range(1,10)]\n",
    "        self.col_whole = self.col_front + self.col_back + self.col_full\n",
    "        \n",
    "        self.df = df[['type', 'quant', 'frontness', \"comple\"] + self.col_whole]\n",
    "\n",
    "        self.category = ['type', 'quant', 'frontness', \"comple\"]\n",
    "        self.model_type = ['normal', 'dependency']\n",
    "        self.quant_type = [\"every\", \"less than 5\", \"more than 5\", \"no\"]\n",
    "        self.front_type = [\"front\", \"back\"]\n",
    "        self.compl_type = [\"and\", \"or\"]\n",
    "        \n",
    "    \n",
    "    def process(self, *, output_url, \n",
    "                want_col = \"all\", # analyze which columns \n",
    "                of_model = None, of_quant = None, of_front = None, of_compl = None,\n",
    "                per_model = None, per_quant = None, per_front = None, per_compl = None): \n",
    "        # analyze per: 1) certain values in each type, 2) whole values in each type, 3) no criterias of the type\n",
    "        # by should be subset of per\n",
    "        \n",
    "        if want_col == \"all\": want_col = self.col_front + self.col_back + self.col_full  \n",
    "        else:                 want_col = want_col\n",
    "\n",
    "\n",
    "\n",
    "        crit_of = []\n",
    "        if of_model != None: crit_of.append(\"type\")\n",
    "        if of_quant != None: crit_of.append(\"quant\")\n",
    "        if of_front != None: crit_of.append(\"frontness\")\n",
    "        if of_compl != None: crit_of.append(\"comple\")\n",
    "\n",
    "        if of_model == True: of_model = self.model_type\n",
    "        if of_quant == True: of_quant = self.quant_type\n",
    "        if of_front == True: of_front = self.front_type\n",
    "        if of_compl == True: of_compl = self.compl_type\n",
    "        of_all = [i for i in [of_model, of_quant, of_front, of_compl] if i != None]\n",
    "\n",
    "        filters = self.df.apply(lambda x: all(x[i] in j for i, j in zip(crit_of, of_all)), axis = 1)\n",
    "        my_df = self.df.copy().loc[filters]\n",
    "\n",
    "        # TODO: by_xx should be subset of per_xx\n",
    "\n",
    "        crit_per = []\n",
    "        if per_model == True: crit_per.append(\"type\")\n",
    "        if per_quant == True: crit_per.append(\"quant\")\n",
    "        if per_front == True: crit_per.append(\"frontness\")\n",
    "        if per_compl == True: crit_per.append(\"comple\")\n",
    "        crit_all = {\"type\": of_model, \"quant\": of_quant, \"frontness\": of_front, \"comple\": of_compl}\n",
    "        crit_other = set(crit_all.keys()) - set(crit_per)\n",
    "        crit_other = {key: crit_all[key] for key in crit_other}\n",
    "\n",
    "        print(crit_other)\n",
    "        \n",
    "        out = my_df.groupby(crit_per).agg(lambda x: (self.operation_comple_compare(x, crit_other)))\n",
    "        filters = CheckUtils.checkSeries(out, types = \"column\")\n",
    "        \n",
    "        \n",
    "\n",
    "        return self.split_database(out[filters])\n",
    "        \n",
    "\n",
    "    def operation_comple_compare(self, x, crit_other):\n",
    "        if x.name in crit_other.keys():\n",
    "            return np.nan\n",
    "        \n",
    "        # return round(x.iloc[0] / x.iloc[1],2)\n",
    "        if (x.iloc[0] == 0) and (x.iloc[1]  == 0):   return np.nan\n",
    "        else:                                       return x.iloc[0] < 0.95 * x.iloc[1] # and < or\n",
    "\n",
    "    def split_database(self, df):\n",
    "        return (df[filter(lambda x: x.startswith(\"f\"), df.columns)],\n",
    "               df[filter(lambda x: x.startswith(\"b\"), df.columns)],\n",
    "               df[filter(lambda x: x.startswith(\"n\"), df.columns)])\n",
    "\n",
    "    def slice_db(self, *, output_url, \n",
    "                want_col = \"all\", # analyze which columns \n",
    "                of_model = None, of_quant = None, of_front = None, of_compl = None):\n",
    "                \n",
    "        # analyze per: 1) certain values in each type, 2) whole values in each type, 3) no criterias of the type\n",
    "        # by should be subset of per\n",
    "        \n",
    "        if want_col == \"all\": want_col = self.col_front + self.col_back + self.col_full  \n",
    "        else:                 want_col = want_col\n",
    "\n",
    "\n",
    "\n",
    "        crit_of = []\n",
    "        if of_model != None: crit_of.append(\"type\")\n",
    "        if of_quant != None: crit_of.append(\"quant\")\n",
    "        if of_front != None: crit_of.append(\"frontness\")\n",
    "        if of_compl != None: crit_of.append(\"comple\")\n",
    "\n",
    "        if of_model == True: of_model = self.model_type\n",
    "        if of_quant == True: of_quant = self.quant_type\n",
    "        if of_front == True: of_front = self.front_type\n",
    "        if of_compl == True: of_compl = self.compl_type\n",
    "        of_all = [i for i in [of_model, of_quant, of_front, of_compl] if i != None]\n",
    "\n",
    "        filters = self.df.apply(lambda x: all(x[i] in j for i, j in zip(crit_of, of_all)), axis = 1)\n",
    "        my_df = self.df.copy().loc[filters]\n",
    "\n",
    "        return self.split_database_to_slice_db(my_df)               \n",
    "    \n",
    "    def split_database_to_slice_db(self, df):\n",
    "        return (df[self.category+ list(filter(lambda x: x.startswith(\"f\"), df.columns))],\n",
    "               df[self.category+ list(filter(lambda x: x.startswith(\"b\"), df.columns))],\n",
    "               df[self.category+ list(filter(lambda x: x.startswith(\"n\"), df.columns))])\n",
    "\n",
    "    \n",
    "class Dataloader():\n",
    "    def __init__(self, dff, dfb, dfn):\n",
    "        self.dff = dff[['f4','f5','f6']]\n",
    "        self.dfb = dfb[['b4','b5','b6']]\n",
    "        self.dfn = dfn[['n4','n5','n6']]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dff)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.dff.iloc[idx].values, self.dfb.iloc[idx].values, self.dfn.iloc[idx].values\n",
    "\n",
    "\n",
    "class ToyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ToyModel, self).__init__()\n",
    "        self.linear = nn.Linear(in_features=2, out_features=1, bias = False)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.linear(inputs)\n",
    "    \n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    base = \"~/dependency_bert/semantics_result/processed/\"\n",
    "    url = \"semantics_result.csv\"\n",
    "\n",
    "    wrapper = []\n",
    "    for whole in ['FullofWhole', \"FullofConcerns\"]:\n",
    "        processor = Processor(base, url, whole)\n",
    "        # out = processor.slice_db(output_url=\"temp.csv\", want_col=\"all\", \n",
    "        #                   of_model=['normal'], of_quant=True, of_front=True, of_compl=True,\n",
    "        #                   per_model=True, per_quant=True, per_front=True, per_compl=False)\n",
    "        out_df = processor.slice_db(output_url=\"temp.csv\", want_col=\"all\", \n",
    "                                of_model=True, of_quant=True, of_front=True, of_compl=True)\n",
    "        \n",
    "        dataloader = Dataloader(out_df[0], out_df[1], out_df[2])\n",
    "\n",
    "        model = ToyModel().to(device)\n",
    "        optimizer = optim.Adam(model.parameters())\n",
    "        criterion = MSELoss()\n",
    "\n",
    "\n",
    "        result_weight = []\n",
    "        for idx,i in enumerate(dataloader):\n",
    "            f456, b456, n456 = i\n",
    "            src = torch.tensor([f456,b456], dtype=torch.float32).transpose(1,0).to(device)\n",
    "            tgt = torch.tensor(n456, dtype=torch.float32).view(-1,1).to(device)\n",
    "\n",
    "            for _ in range(10000):\n",
    "                \n",
    "            \n",
    "                optimizer.zero_grad()\n",
    "                out = model(src)\n",
    "                loss = criterion(out, tgt)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            res_weight = model.linear.weight.cpu().detach().numpy().reshape(-1)\n",
    "            res_weight = [round(i,2) for i in (res_weight / sum(res_weight)).tolist()]       \n",
    "            result_weight.append(res_weight)\n",
    "            print(f\"{idx} done\")\n",
    "        \n",
    "        x = pd.DataFrame(result_weight)\n",
    "        x = pd.concat([out_df[0][processor.category], x], axis = 1)\n",
    "        x = pd.concat([pd.Series([whole] * 32), x], axis = 1)\n",
    "        wrapper.append(x)\n",
    "    final = pd.concat(wrapper, axis= 0)\n",
    "    \n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper[0].columns = [\"attention_window\", \"model_type\", \"quantifier_type\", \"frontedness\", \"frontedness\", \"connectives\", \"preceding_proportion\", \"reverse_proportaion\"]\n",
    "wrapper[0].iloc[:,[0,1,2,3,5,6,7]].to_csv(\"~/dependency_bert/semantics_result/processed/whole.csv\")\n",
    "wrapper[1].columns = [\"attention_window\", \"model_type\", \"quantifier_type\", \"frontedness\", \"frontedness\", \"connectives\", \"preceding_proportion\", \"reverse_proportaion\"]\n",
    "wrapper[1].iloc[:,[0,1,2,3,5,6,7]].to_csv(\"~/dependency_bert/semantics_result/processed/partial.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
