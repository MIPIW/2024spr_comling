{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "17d99620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from collections import Counter\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "b6d31b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "afab08dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessFiles():\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def open_process(self, fileurl, args1_length, split_to, merge_type):\n",
    "        with open(fileurl, \"r\") as f:\n",
    "            lines = [i.strip() for i in f.readlines()]\n",
    "        lines = [self._process_single_line(i, args1_length, split_to, merge_type) for i in lines]\n",
    "        \n",
    "        sentences = [i[0] for i in lines]\n",
    "        labels = [i[1] for i in lines]\n",
    "\n",
    "        return sentences, labels\n",
    "\n",
    "    def _process_single_line(self, line, args1_length, split_to, merge_type):\n",
    "\n",
    "        normal_split = line.strip('.').split(\" \")\n",
    "        before_comp_after = line.strip(\".\").split(f\" {split_to} \")\n",
    "        \n",
    "        before_comp = before_comp_after[0].split(\" \")\n",
    "        after_comp = before_comp_after[1].split(\" \")\n",
    "        \n",
    "        component_a = before_comp[:-args1_length]\n",
    "        component_arg1 = before_comp[-args1_length:]\n",
    "        component_compl = split_to\n",
    "        component_arg2 = after_comp # as the sentence ends with the arguments\n",
    "\n",
    "        labels_space = [0 for _ in component_a] + [1 for _ in component_arg1] + [2] + [3 for _ in component_arg2]  # 2: split to\n",
    "        labels_space.append(0) # 온점\n",
    "\n",
    "        return line, labels_space # str, list\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "026a1301",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Attention_dataset(Dataset):\n",
    "    def __init__(self, sentences, labels_space, tokenizer, merge_type, max_length = 32):\n",
    "        self.sentences = sentences\n",
    "        self.labels_space = labels_space\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.merge_type = merge_type\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        sentence = self.sentences[index]\n",
    "        label_space = self.labels_space[index]\n",
    "        \n",
    "        inputs = self.tokenizer(sentence, padding = \"max_length\", max_length = self.max_length, truncation = True, return_tensors = \"pt\")\n",
    "        label_tokenize, label_tokenize_expand = self._get_labels_tokenize(sentence, label_space)\n",
    "        mask = self._make_mask(label_tokenize)\n",
    "\n",
    "        inputs['attention_mask'] = torch.tensor(mask, requires_grad=False)\n",
    "        inputs = {key: value.requires_grad_(False) for key, value in inputs.items()}\n",
    "        label_tokenize_expand = torch.tensor(label_tokenize_expand, requires_grad=False)\n",
    "\n",
    "        return inputs, label_tokenize_expand\n",
    "    \n",
    "    \n",
    "    def _get_labels_tokenize(self, sentence, label_space):\n",
    "        tokens = self.tokenizer.tokenize(sentence)\n",
    "        \n",
    "        #[a ##a ##a b]: [0,1,1,0], [0,1,2,2], [0,1,2,3], [0,0,0,1]\n",
    "        reduction_index = [1 if token.startswith(\"##\") else 0 for token in tokens]\n",
    "        cum_reduction_index = np.array([sum(reduction_index[:i+1]) for i, _ in enumerate(tokens)])\n",
    "        temp = np.array([i for i, _ in enumerate(tokens)])\n",
    "        index = temp - cum_reduction_index\n",
    "\n",
    "        # [typea,typea,typea, typeb]\n",
    "        label_tokenize = [label_space[i] for i in index]\n",
    "        label_tokenize_expand = [0] + label_tokenize + [0] + [4] * (self.max_length - (len(label_tokenize) + 2))\n",
    "\n",
    "        return label_tokenize, label_tokenize_expand\n",
    "    \n",
    "    def _make_mask(self, label_tokenize):\n",
    "        lst = []\n",
    "        if self.merge_type == \"front\":\n",
    "            for idx_row, label_row in enumerate([0] + label_tokenize + [0]):\n",
    "                if (label_row == 1) or (label_row == 2):\n",
    "                    row_mask = [0] + [1 if (label_col == 1) or (label_col == 2) else 0  for _, label_col in enumerate(label_tokenize)] + [0] + [0] * (self.max_length - (len(label_tokenize) + 2))\n",
    "                else:\n",
    "                    row_mask = [1] + [1                                                 for _, label_col in enumerate(label_tokenize)] + [1] + [0] * (self.max_length - (len(label_tokenize) + 2))\n",
    "                \n",
    "                lst.append(row_mask)\n",
    "        \n",
    "        if self.merge_type == \"back\":\n",
    "            for idx_row, label_row in enumerate([0] + label_tokenize + [0]):\n",
    "                if (label_row == 2) or (label_row == 3):\n",
    "                    row_mask = [0] + [1 if (label_col == 2) or (label_col == 3) else 0  for _, label_col in enumerate(label_tokenize)] + [0] + [0] * (self.max_length - (len(label_tokenize) + 2))\n",
    "                else:\n",
    "                    row_mask = [1] + [1                                                 for _, label_col in enumerate(label_tokenize)] + [1] + [0] * (self.max_length - (len(label_tokenize) + 2))\n",
    "                \n",
    "                lst.append(row_mask)\n",
    "        \n",
    "        if self.merge_type == \"none\":\n",
    "            for idx_row, label_row in enumerate([0] + label_tokenize + [0]):\n",
    "                if (label_row == 2):\n",
    "                    row_mask = [0] + [1 if (label_col == 1) or (label_col == 2) or (label_col == 3) else 0  for _, label_col in enumerate(label_tokenize)] + [0] + [0] * (self.max_length - (len(label_tokenize) + 2))\n",
    "                else:\n",
    "                    row_mask = [1] + [1 for _, label_col in enumerate(label_tokenize)] + [1] + [0] * (self.max_length - (len(label_tokenize) + 2))\n",
    "\n",
    "                # row_mask = [1] + [1 for _, label_col in enumerate(label_tokenize)] + [1] + [0] * (self.max_length - (len(label_tokenize) + 2))\n",
    "\n",
    "                lst.append(row_mask)\n",
    "\n",
    "        for _ in range(self.max_length - len(lst)):\n",
    "            lst.append([0] * self.max_length)\n",
    "        \n",
    "\n",
    "        custom_mask = lst    \n",
    "        return custom_mask  \n",
    "\n",
    "\n",
    "\n",
    "class Collator():\n",
    "    def __init__(self, max_length):\n",
    "        self.max_length = max_length\n",
    "        pass\n",
    "\n",
    "    def __call__(self, samples):\n",
    "        # sample = samples[8] # a single input\n",
    "        data = [sample[0] for sample in samples]\n",
    "        labels = [sample[1] for sample in samples]\n",
    "        \n",
    "        data, labels = {\"input_ids\": torch.stack([sample[\"input_ids\"].view(-1).contiguous() for sample in data]).to(device), \n",
    "            \"attention_mask\": torch.stack([sample[\"attention_mask\"]for sample in data]).to(device)}, torch.stack(labels).to(device)\n",
    "\n",
    "        return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "10975ac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1 == 2) or (1 == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "785c1019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistical analysis of attention score\n",
    "#calculate args1, args2, and, others 4*4 matrix, with group getting attention: sum, group sending attetnion: average\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "class Postprocessor():\n",
    "    def __init__(self, max_length, device = device):\n",
    "        self.max_length = max_length\n",
    "        self.device = device\n",
    "        pass\n",
    "\n",
    "    def process(self, whole_attention, labels_tokenize): # whole attention: layers, batch, x, y\n",
    "\n",
    "        output_lst = []\n",
    "        num_batch = labels_tokenize.size()[0]\n",
    "        \n",
    "        for index in range(num_batch): #per each sentence in the batch \n",
    "            \n",
    "            label_tokenize = labels_tokenize[index].cpu().detach().tolist() # seq_len\n",
    "            counter = Counter(label_tokenize)\n",
    "            # print(duplicated_index, duplicated_average_index, num_labels, sep=\"\\n\")\n",
    "            # print(\"\\n\\n\")\n",
    "\n",
    "            matrix_A = torch.zeros((self.max_length, len(counter)), requires_grad=False).to(device) # for sum \n",
    "            \n",
    "            matrix_n = self.max_length\n",
    "            matrix_m = len(counter)\n",
    "\n",
    "            for column in range(matrix_m): # for each column\n",
    "                for row, label in zip(range(matrix_n), label_tokenize):\n",
    "                    if label == column:\n",
    "                        matrix_A[row][column] = 1\n",
    "            \n",
    "            # return torch.einsum(\"sr, hrc, ca -> hsa\", matrix_A.T, whole_attention[0][index, ...], matrix_A), matrix_A, whole_attention[0][index, ...]\n",
    "        \n",
    "            x = torch.stack([torch.einsum(\"sr, hrc, ca -> hsa\", matrix_A.T, layer_attention[index, ...], matrix_A) for layer_attention in whole_attention])\n",
    "            output_lst.append(x)\n",
    "\n",
    "        return torch.stack(output_lst) # batch, layers, head, seq_len, seq_len\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29fbd08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "e42771d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main(*, fileurl, args1_length, split_to, bertModel, bertTokenizer, max_length, merge_type):\n",
    "\n",
    "    bertModel.eval()\n",
    "    bertModel.to(device)\n",
    "\n",
    "    processfiles = ProcessFiles()\n",
    "    collator = Collator(max_length)\n",
    "    postprocessor = Postprocessor(max_length)\n",
    "\n",
    "    sentences, labels_space = processfiles.open_process(fileurl, args1_length, split_to, merge_type)\n",
    "    dataset = Attention_dataset(sentences, labels_space, bertTokenizer, merge_type, max_length)\n",
    "    dataloader = DataLoader(dataset, batch_size = 128, shuffle = False, collate_fn=collator)\n",
    "\n",
    "\n",
    "    for data in dataloader: # single batch to process whole sentence\n",
    "        \n",
    "        inputs = data[0]\n",
    "        labels = data[1]\n",
    "\n",
    "        out = bertModel(**inputs, output_attentions = True)\n",
    "        out = postprocessor.process(out.attentions, labels)\n",
    "        # return inputs, out\n",
    "        out = torch.round(out.mean(dim = (0,1,2)), decimals=3)\n",
    "        out = list(out[1:4, 1:4].reshape(-1).contiguous().cpu().detach().numpy())\n",
    "\n",
    "    type = fileurl.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "    return type, out\n",
    "\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "ed9dac3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at /home/hyohyeongjang/dependency_bert/checkpoint_output_dependency/checkpoint-dependency-high and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done_normal_front\n",
      "done_normal_back\n",
      "done_normal_none\n",
      "done_dependency_front\n",
      "done_dependency_back\n",
      "done_dependency_none\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # lookup attention score of each tokens\n",
    "    ## with base model, with custom model, with random model\n",
    "    checkpoint = \"google-bert/bert-base-uncased\"\n",
    "    model_type = \"normal\"\n",
    "    checkpoint = \"/home/hyohyeongjang/dependency_bert/checkpoint_output_dependency/checkpoint-dependency-high\"\n",
    "    model_type = \"dependency\"\n",
    "    bertTokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "    bertModel = AutoModel.from_pretrained(checkpoint)\n",
    "    base = \"/home/hyohyeongjang/dependency_bert/semantic_dataset/\"\n",
    "    \n",
    "    fileurl = [base+i for i in sorted(os.listdir(base))]\n",
    "    category = [(\"every\", \"front\", \"and\"),\n",
    "                (\"every\", \"front\", \"or\"),\n",
    "                (\"less than 5\", \"front\", \"and\"),\n",
    "                (\"less than 5\", \"front\", \"or\"),\n",
    "                (\"more than 5\", \"front\", \"and\"),\n",
    "                (\"more than 5\", \"front\", \"or\"),\n",
    "                (\"every\", \"back\", \"and\"),\n",
    "                (\"less than 5\", \"back\", \"and\"),\n",
    "                (\"more than 5\", \"back\", \"and\"),\n",
    "                (\"no\", \"back\", \"and\"),\n",
    "                (\"every\", \"back\", \"or\"),\n",
    "                (\"less than 5\", \"back\", \"or\"),\n",
    "                (\"more than 5\", \"back\", \"or\"),\n",
    "                (\"no\", \"back\", \"or\"),\n",
    "                (\"no\", \"front\", \"and\"),\n",
    "                (\"no\", \"front\", \"or\"),                \n",
    "                ]\n",
    "    \n",
    "    args1_length = [2,2,4,4,4,4,1,1,1,1,1,1,1,1,2,2]\n",
    "    split_to = [i.split(\"_\")[-2] for i in fileurl]\n",
    "    model_types = [\"normal\", \"dependency\"]\n",
    "    model_checkpoints = [\"google-bert/bert-base-uncased\", \"/home/hyohyeongjang/dependency_bert/checkpoint_output_dependency/checkpoint-dependency-high\"]\n",
    "    \n",
    "    for model_type, model_checkpoint in zip(model_types, model_checkpoints):\n",
    "        for merge_type, m in zip([\"front\", \"back\", \"none\"], [\"f\", \"b\", \"n\"]):    \n",
    "\n",
    "            outs = []\n",
    "            files = []\n",
    "            for f,l,s, c in zip(fileurl,args1_length,split_to, category):\n",
    "                file, out = main(fileurl=f, \n",
    "                    args1_length=l,\n",
    "                    split_to=s,\n",
    "                    bertModel=bertModel,\n",
    "                    bertTokenizer=bertTokenizer,\n",
    "                    max_length=32,\n",
    "                    merge_type=merge_type)\n",
    "                outs.append(out)\n",
    "                files.append(file)\n",
    "\n",
    "            category_this = [[model_type] + list(i) for i in category]\n",
    "            category_this = pd.DataFrame(category_this)\n",
    "            category_this.columns = [\"type\", \"quant\", \"frontness\", \"comple\"]            \n",
    "            \n",
    "            df = pd.DataFrame(outs)\n",
    "            df.columns = [f\"{m}{i}\" for i in range(1,10)]\n",
    "            df = pd.concat([category_this, df], axis = 1)\n",
    "\n",
    "            df.to_csv(f\"/home/hyohyeongjang/dependency_bert/semantics_result/df_{model_type}_{merge_type}_FullofWhole.csv\")\n",
    "            print(f\"done_{model_type}_{merge_type}\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b51eac5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>every_and_n</th>\n",
       "      <td>0.114</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>every_or_n</th>\n",
       "      <td>0.113</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lessthan5_and_n</th>\n",
       "      <td>0.095</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lessthan5_or_n</th>\n",
       "      <td>0.093</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>morethan5_and_n</th>\n",
       "      <td>0.092</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>morethan5_or_n</th>\n",
       "      <td>0.091</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_and_every</th>\n",
       "      <td>0.105</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_and_lessthan5</th>\n",
       "      <td>0.088</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_and_morethan5</th>\n",
       "      <td>0.085</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_and_no</th>\n",
       "      <td>0.111</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_or_every</th>\n",
       "      <td>0.103</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_or_lessthan5</th>\n",
       "      <td>0.087</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_or_morethan5</th>\n",
       "      <td>0.084</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_or_no</th>\n",
       "      <td>0.110</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_and_n</th>\n",
       "      <td>0.110</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_or_n</th>\n",
       "      <td>0.109</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     0      1      2      3      4      5      6      7      8\n",
       "every_and_n      0.114  0.100  0.099  0.100  0.095  0.125  0.106  0.099  0.111\n",
       "every_or_n       0.113  0.108  0.097  0.101  0.102  0.124  0.105  0.107  0.110\n",
       "lessthan5_and_n  0.095  0.073  0.080  0.086  0.078  0.107  0.092  0.076  0.090\n",
       "lessthan5_or_n   0.093  0.079  0.079  0.086  0.084  0.108  0.091  0.082  0.089\n",
       "morethan5_and_n  0.092  0.073  0.076  0.083  0.078  0.101  0.090  0.077  0.087\n",
       "morethan5_or_n   0.091  0.079  0.075  0.082  0.085  0.103  0.089  0.083  0.086\n",
       "n_and_every      0.105  0.118  0.101  0.100  0.098  0.107  0.095  0.096  0.114\n",
       "n_and_lessthan5  0.088  0.092  0.087  0.085  0.077  0.087  0.081  0.073  0.093\n",
       "n_and_morethan5  0.085  0.090  0.082  0.083  0.078  0.084  0.079  0.073  0.089\n",
       "n_and_no         0.111  0.116  0.096  0.104  0.096  0.105  0.100  0.095  0.109\n",
       "n_or_every       0.103  0.127  0.099  0.101  0.104  0.106  0.095  0.103  0.112\n",
       "n_or_lessthan5   0.087  0.101  0.085  0.087  0.082  0.087  0.082  0.078  0.091\n",
       "n_or_morethan5   0.084  0.099  0.081  0.084  0.084  0.083  0.079  0.079  0.088\n",
       "n_or_no          0.110  0.126  0.094  0.105  0.104  0.103  0.100  0.103  0.107\n",
       "no_and_n         0.110  0.101  0.102  0.099  0.095  0.129  0.100  0.100  0.117\n",
       "no_or_n          0.109  0.110  0.099  0.099  0.103  0.127  0.099  0.108  0.115"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60ecd16c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467652ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
