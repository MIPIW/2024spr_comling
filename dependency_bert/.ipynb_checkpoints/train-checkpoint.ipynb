{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "521dd1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hyohyeongjang/.local/lib/python3.9/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math, os, re, argparse, string\n",
    "from matplotlib import pyplot as plt\n",
    "from functools import partial\n",
    "from operator import itemgetter\n",
    "import string\n",
    "from itertools import groupby\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import spacy\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.multiprocessing\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import AutoModelForMaskedLM, AutoConfig, AutoTokenizer\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "import datasets, evaluate\n",
    "\n",
    "import warnings\n",
    "import pickle\n",
    "\n",
    "from pathlib import Path\n",
    "import jsonlines\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b929aa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class IOUtils():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def existsFile(file_url):\n",
    "        \n",
    "        import os\n",
    "        from pathlib import Path\n",
    "\n",
    "        if type(file_url) == str:\n",
    "            url = Path(file_url).resolve()\n",
    "        else:\n",
    "            url = file_url.resolve()\n",
    "        \n",
    "        if url.exists():\n",
    "            c = 1\n",
    "            while True:\n",
    "                if Path(str(url.parent) + \"/\" + str(url.stem) + f\"_{c}\" + str(url.suffix)).exists():\n",
    "                    c += 1\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            return Path(str(url.parent) + \"/\" + str(url.stem) + f\"_{c}\" + str(url.suffix))\n",
    "        \n",
    "        print(f\"The url of the fIle to be saved is {url}\")\n",
    "        return url\n",
    "    \n",
    "    @staticmethod\n",
    "    def recentFile(file_url):\n",
    "        \n",
    "        import os\n",
    "        from pathlib import Path\n",
    "\n",
    "        if type(file_url) == str:\n",
    "            url = Path(file_url).resolve()\n",
    "        else:\n",
    "            url = file_url.resolve()\n",
    "\n",
    "        if url.exists():\n",
    "            c = 1\n",
    "            while True:\n",
    "                if Path(str(url.parent) + \"/\" + str(url.stem) + f\"_{c}\" + str(url.suffix)).exists():\n",
    "                    c += 1\n",
    "                else:\n",
    "                    break\n",
    "        else:\n",
    "            c = 0\n",
    "        if c == 0:\n",
    "            return f\"File not exists in {url}\"\n",
    "\n",
    "        if c == 1:\n",
    "            print(f'The url of the fIle to be loaded is {Path(str(url.parent) + \"/\" + str(url.stem + str(url.suffix)))}')\n",
    "            return Path(str(url.parent) + \"/\" + str(url.stem + str(url.suffix)))\n",
    "        else:\n",
    "            print(f'The url of the fIle to be loaded is {Path(str(url.parent) + \"/\" + str(url.stem + f\"_{c-1}\" + str(url.suffix)))}')\n",
    "            return Path(str(url.parent) + \"/\" + str(url.stem + f\"_{c-1}\" + str(url.suffix)))\n",
    "\n",
    "    @staticmethod\n",
    "    def checkpoint_save(file_path, data, \n",
    "                        data_type = \"dataFrame\", \n",
    "                        file_type = \"csv\", \n",
    "                        index_dataFrame = False):\n",
    "        import json\n",
    "\n",
    "        save_path = IOUtils.existsFile(file_path)\n",
    "        \n",
    "        if data_type == \"dataFrame\" or data_type == \"series\":\n",
    "            import pandas as pd\n",
    "\n",
    "            if file_type == \"csv\":\n",
    "                data.to_csv(save_path, index = index_dataFrame, encoding = 'utf-8')\n",
    "                                \n",
    "        \n",
    "        if data_type == \"list\":\n",
    "            if file_type == \"txt\":\n",
    "                with open(save_path, \"w\", encoding = 'utf-8') as f:\n",
    "                    for item in data:\n",
    "                        f.write(item)\n",
    "                        f.write(\"\\n\")\n",
    "\n",
    "                \n",
    "            if file_type == \"jsonl\":\n",
    "                \n",
    "                with open(save_path, \"w\", encoding = 'utf-8') as f:\n",
    "                    for item in data.items():\n",
    "                        f.write(json.dumps(item))\n",
    "                        f.write(\"\\n\")\n",
    "\n",
    "        \n",
    "        if data_type == \"dict\":\n",
    "            if file_type == \"json\":\n",
    "                \n",
    "                \n",
    "                with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(data, f, indent = \"\\t\")\n",
    "        \n",
    "        return data # for the neatness of the code\n",
    "    \n",
    "    @staticmethod\n",
    "    def checkpoint_load(file_path, \n",
    "                        data_type = \"dataFrame\", \n",
    "                        file_type = \"csv\"):\n",
    "        load_path = IOUtils.recentFile(file_path)\n",
    "        \n",
    "        if data_type == \"dataFrame\" or data_type == \"series\":\n",
    "            import pandas as pd\n",
    "\n",
    "            if file_type == \"csv\":\n",
    "                out = pd.read_csv(load_path)\n",
    "                return out\n",
    "\n",
    "        if data_type == \"list\":\n",
    "            if file_type == \"txt\":\n",
    "                out = None\n",
    "\n",
    "                with open(load_path, \"r\", encoding = 'utf-8') as f:\n",
    "                    out = [i.strip() for i in f.readlines()]\n",
    "                return out\n",
    "\n",
    "            if file_type == \"jsonl\":\n",
    "                out = []\n",
    "\n",
    "                with open(load_path, \"r\", encoding = 'utf-8') as f:\n",
    "                    for line in f:\n",
    "                        out.append(json.loads(line))\n",
    "\n",
    "                return out\n",
    "\n",
    "        \n",
    "        if data_type == \"dict\":\n",
    "            if file_type == \"json\":\n",
    "                \n",
    "                out = None\n",
    "                \n",
    "                with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(data, f, indent = \"\\t\")\n",
    "                \n",
    "                with open(load_path, \"r\") as f:\n",
    "                    out = json.loads(f)\n",
    "                \n",
    "                return out\n",
    "            \n",
    "\n",
    "\n",
    "class ParallelizingUtils():\n",
    "    \n",
    "    import multiprocessing\n",
    "    \n",
    "\n",
    "    def __init__(self):\n",
    "        self.type = None\n",
    "        self.func = None\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def do_series(self, series, num_cores, pre_assign = False):\n",
    "\n",
    "        from multiprocessing import Pool\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "\n",
    "        if num_cores == 1: ##############somewhere error occurs\n",
    "            if not pre_assign:\n",
    "                return self._assign_map(series)\n",
    "            else:\n",
    "                return self.func(series)\n",
    "\n",
    "        se_split = np.array_split(series, num_cores)\n",
    "        pool = Pool(num_cores)\n",
    "        if not pre_assign:\n",
    "            df = pd.concat(pool.map(self._assign_map, se_split))\n",
    "        else:\n",
    "            df = pd.concat(pool.map(self.func, se_split))\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _assign_map(self, serie):\n",
    "        return serie.progress_map(self.func)\n",
    "    \n",
    "    def _assign_apply(self, df, axis):\n",
    "        return df.progress_apply(self.func, axis = axis)\n",
    "\n",
    "    def change_function(self, func):\n",
    "        self.func = func\n",
    "\n",
    "    def do_dataFrame(self, df, num_cores, axis = None, pre_assign = False):\n",
    "        from multiprocessing import Pool\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        from functools import partial\n",
    "        \n",
    "        if num_cores == 1:\n",
    "            if not pre_assign:\n",
    "                if axis == None:\n",
    "                    return valueError(\"axis needed\")\n",
    "                return self._assign_apply(df, axis = axis)\n",
    "            else:\n",
    "                return self.func(df)\n",
    "\n",
    "        se_split = np.array_split(df, num_cores)\n",
    "        pool = Pool(num_cores)\n",
    "        if not pre_assign:\n",
    "            \n",
    "            if axis == None:\n",
    "                    return valueError(\"axis needed\")\n",
    "            \n",
    "            f = partial(self._assign_apply, axis = axis)\n",
    "            df = pd.concat(pool.map(f, se_split))\n",
    "            \n",
    "        else:\n",
    "            df = pd.concat(pool.map(self.func, se_split))\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        \n",
    "        return df\n",
    "\n",
    "\n",
    "class CheckUtils():\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "    \n",
    "    # def checkValue(func):\n",
    "    #     from functools import partial\n",
    "    #     partial \n",
    "    @staticmethod\n",
    "    def checkSeries(serdf, \n",
    "                 isNan = True, \n",
    "                 isEmpty = True, \n",
    "                 isInf = True,\n",
    "                    isNotInstance = None,\n",
    "                    isNotIn = False,\n",
    "                    isNotInVal = None\n",
    "                 ):\n",
    "        \n",
    "\n",
    "        \n",
    "        def _check_subsequent(x, \n",
    "                              isNotInstance = isNotInstance, \n",
    "                              isNan = isNan, \n",
    "                              isEmpty = isEmpty, \n",
    "                              isInf = isInf, \n",
    "                              isnotIn = isNotIn, \n",
    "                              isNotInVal = isNotInVal):\n",
    "            out = False\n",
    "            \n",
    "            if isNotInstance is not None:\n",
    "                x1 = (not isinstance(x, isNotInstance))\n",
    "            else:\n",
    "                x1 = False\n",
    "\n",
    "            if isinstance(x, (int, float, complex)):\n",
    "                x2 = pd.isna(x) if isNan else False\n",
    "                x3 = math.isinf(x) if isInf else False\n",
    "                x4 = False\n",
    "            \n",
    "            elif x == None:\n",
    "                x2 = False\n",
    "                x3 = False\n",
    "                x4 = True if isEmpty else False\n",
    "            \n",
    "            else:\n",
    "                x2 = False\n",
    "                x3 = False\n",
    "                x4 = (len(x) == 0) if isEmpty else False\n",
    "                \n",
    "            x5 = isNotInVal not in x if isNotIn else False\n",
    "\n",
    "            return out | x1 | x2 | x3 | x4 | x5\n",
    "        \n",
    "        \n",
    "        if isinstance(serdf, pd.Series):\n",
    "            return serdf.progress_map(lambda x: _check_subsequent(x))\n",
    "        \n",
    "        elif isinstance(serdf, pd.DataFrame):\n",
    "            out  = serdf.map(lambda x: _check_subsequent(x)) # cellwise\n",
    "\n",
    "            return out.apply(lambda x: any(x), axis = 1)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"this is not either series or dataframe.\")\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def isEmpty(dataframe) -> bool:\n",
    "    \n",
    "        if len(dataframe.index) == 0:\n",
    "            return True\n",
    "    \n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bdc774b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing():\n",
    "    \n",
    "    def __init__(self, spacy_args):\n",
    "        \n",
    "        self.parser = spacy.load(spacy_args)\n",
    "    \n",
    "    \n",
    "    def preprocess_assign(self, inputs):\n",
    "        return inputs.progress_map(self.preprocess)\n",
    "    \n",
    "    def preprocess(self, single_inputs):\n",
    "        \n",
    "        \n",
    "    \n",
    "        out = re.sub(\"'\", \"\", single_inputs)\n",
    "        out = re.sub(r'[^\\w\\s\\d]', '' , out)\n",
    "        out = re.sub(r\"\\s{2,}\", \" \", out)\n",
    "        return out\n",
    "    \n",
    "    def process_assign(self, inputs):\n",
    "        return inputs.progress_map(self.process)\n",
    "    \n",
    "    def process(self, single_inputs):\n",
    "        return self.parser(single_inputs)\n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7df9295",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Masking():\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass                        \n",
    "        \n",
    "        \n",
    "    @staticmethod\n",
    "    def extract_mask_assign(df_source, pure_dependency):\n",
    "        return df_source.apply(lambda x: Masking.extract_mask(x['preprocessed'], x['spacy_processed'], pure_dependency), axis = 1)\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_mask(original_input, parsed_input, pure_dependency):\n",
    "        \n",
    "        def _omit_subword(original_sentence, parsed_input):\n",
    "            inputs = [x.text for x in parsed_input]\n",
    "            original = original_sentence.split(\" \")\n",
    "\n",
    "\n",
    "            if len(original) == len(inputs):\n",
    "                return torch.arange(len(parsed_input) , dtype = torch.int , requires_grad = False)\n",
    "            \n",
    "            else:\n",
    "\n",
    "                index_original = 0\n",
    "                index_inputs = 0\n",
    "                sets = set()\n",
    "                sets_fine = set()\n",
    "\n",
    "                while True:\n",
    "                    if index_inputs == len(inputs) or index_original == len(original):\n",
    "                        break\n",
    "\n",
    "                    else:\n",
    "                        if original[index_original].startswith(inputs[index_inputs]):\n",
    "                            sets_fine.add(index_original)\n",
    "                            \n",
    "                            index_original += 1\n",
    "                            index_inputs += 1\n",
    "                            \n",
    "                        else:\n",
    "                            sets.add(index_inputs)\n",
    "                            sets.add(index_inputs - 1)\n",
    "\n",
    "                            index_inputs += 1\n",
    "\n",
    "                successive = [list(map(itemgetter(1), g)) for k, g in groupby(enumerate(list(sets)), lambda i: i[0]-i[1])]\n",
    "\n",
    "                out_idx = sets_fine | set(s[0] for s in successive)\n",
    "                \n",
    "                return torch.tensor(list(out_idx), dtype = torch.int, requires_grad = False)\n",
    "        \n",
    "        try:\n",
    "            root = [token for token in parsed_input if token.head == token][0]\n",
    "            tokens = [token.text for token in parsed_input]\n",
    "            zero_tensor =  torch.zeros(len(tokens), len(tokens), requires_grad=False)\n",
    "\n",
    "            root_left = list(root.lefts)\n",
    "            root_left = set(j for i in root_left for j in i.subtree)\n",
    "\n",
    "            if not pure_dependency:\n",
    "                lst = [[] for _ in range(len(tokens))] \n",
    "                for tokens_idx, token in enumerate(parsed_input):\n",
    "                    if token in root_left:\n",
    "                        if token.head == root:\n",
    "                            lst[tokens_idx] = [t.i for t in list(token.subtree) + [token.head]]\n",
    "                        else:\n",
    "                            lst[tokens_idx] = [t.i for t in list(token.subtree) + [token.head]]\n",
    "                    elif token == root:\n",
    "                        lst[tokens_idx] = [t.i for t in list(token.subtree) + [token.head] if t not in root_left]\n",
    "\n",
    "                    else:\n",
    "                        lst[tokens_idx] = [t.i for t in list(token.subtree) + [token.head]]\n",
    "\n",
    "            else:\n",
    "                lst = [[t.i for t in list(token.subtree) + [token.head] ] for token in parsed_input]       \n",
    "\n",
    "\n",
    "            for i, val in enumerate(lst):\n",
    "                index = torch.tensor(val)\n",
    "                zero_tensor[i].index_fill_(0, index, 1)\n",
    "\n",
    "\n",
    "            output_idx = _omit_subword(original_input, parsed_input)\n",
    "\n",
    "\n",
    "            return zero_tensor[output_idx][:,output_idx]\n",
    "        \n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "    \n",
    "    \n",
    "   \n",
    "            \n",
    "    @staticmethod\n",
    "    def extend_mask_assign(df_source, tokenizer, max_length):\n",
    "        \n",
    "        return df_source.apply(lambda x: Masking._extend_mask(masking = x['spacy_masking'],                                                     \n",
    "                                                       tokenizer = tokenizer,                                                     \n",
    "                                                       original_sentence = x['preprocessed'],                                                     \n",
    "                                                       max_length = max_length), axis = 1)\n",
    "        \n",
    "        \n",
    "    @staticmethod\n",
    "    def _extend_mask(masking, tokenizer, original_sentence, max_length):\n",
    "        try:\n",
    "            out = tokenizer.tokenize(original_sentence,                                \n",
    "                                     padding=\"max_length\",                     \n",
    "                                     max_length=max_length, # sequence들을 max_length까지 패딩                     \n",
    "                                     return_tensors=\"pt\")\n",
    "            out_without_pad = [i for i in out if i != \"[PAD]\"]\n",
    "            pad_len = len(out) - len(out_without_pad)\n",
    "\n",
    "\n",
    "            is_initial = [0 if i.startswith(\"##\") else 1 for i in out_without_pad]\n",
    "            cum_is_initial = [sum(is_initial[:i+1])-1  for i in range(len(out_without_pad))]\n",
    "\n",
    "\n",
    "            subword_mask = torch.tensor([[int(masking[i][j].item()) for j in cum_is_initial] for i in cum_is_initial], requires_grad = False)\n",
    "            pad_zeros = torch.zeros(len(out_without_pad), pad_len, requires_grad = False)\n",
    "\n",
    "            out = torch.cat((subword_mask, pad_zeros), dim = -1)\n",
    "            out.requires_grad = False\n",
    "            zeros = torch.zeros((max_length - out.size()[0], max_length), requires_grad = False)\n",
    "            out = torch.cat([out, zeros], dim = 0)\n",
    "\n",
    "            return out\n",
    "        \n",
    "        except:\n",
    "            return None\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbc3d2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DatasetProducer():\n",
    "    def __init__(self):\n",
    "        self.train_data = None\n",
    "        self.eval_data = None\n",
    "        self.test_data = None\n",
    "        \n",
    "        self.train_dataset = None\n",
    "        self.eval_dataset = None\n",
    "        self.test_dataset = None\n",
    "            \n",
    "    def load_dataset(self):\n",
    "        \n",
    "        \n",
    "        nli_data = datasets.load_dataset(\"nyu-mll/multi_nli\")\n",
    "\n",
    "        train_dataset = pd.Series(nli_data['train']['premise'])\n",
    "        valid_dataset = pd.Series(nli_data['validation_matched']['premise'])\n",
    "        test_dataset = pd.Series(nli_data['validation_mismatched']['premise'])\n",
    "\n",
    "        train_dataset = train_dataset[train_dataset.map(lambda x: 30 < len(x) < 500)]\n",
    "        valid_dataset = valid_dataset[valid_dataset.map(lambda x: 30 < len(x) < 500)]\n",
    "        test_dataset = test_dataset[test_dataset.map(lambda x: 30 < len(x) < 500)]\n",
    "\n",
    "        train_dataset = train_dataset[CheckUtils.checkSeries(train_dataset, \n",
    "                                isNan = True,\n",
    "                                 isEmpty = True,\n",
    "                                 isInf = False,\n",
    "                                 isNotInstance = str).map(lambda x: not x)].reset_index(drop = True)\n",
    "        valid_dataset = valid_dataset[CheckUtils.checkSeries(valid_dataset, \n",
    "                                isNan = True,\n",
    "                                 isEmpty = True,\n",
    "                                 isInf = False,\n",
    "                                 isNotInstance = str).map(lambda x: not x)].reset_index(drop = True)\n",
    "        test_dataset = test_dataset[CheckUtils.checkSeries(test_dataset, \n",
    "                                isNan = True,\n",
    "                                 isEmpty = True,\n",
    "                                 isInf = False,\n",
    "                                 isNotInstance = str).map(lambda x: not x)].reset_index(drop = True)\n",
    "\n",
    "        return train_dataset, valid_dataset, test_dataset\n",
    "    \n",
    "    def load_data_with_custom_mask(self, *,\n",
    "                               dataset, \n",
    "                               slice_len,    \n",
    "                               start_len,\n",
    "                               max_length = 128,  \n",
    "                               num_cores = 2,\n",
    "                               spacy_model = 'en_core_web_sm',    \n",
    "                               tokenizer = \"google-bert/bert-base-uncased\",\n",
    "                               multiprocessing = True,\n",
    "                                \n",
    "                               \n",
    "                              ):\n",
    "\n",
    "        warnings.filterwarnings('ignore')\n",
    "        torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "\n",
    "        if multiprocessing == False:\n",
    "            pass\n",
    "        else:\n",
    "            os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "        \n",
    "        \n",
    "        processor = Preprocessing(spacy_model)\n",
    "        multiprocessor = ParallelizingUtils()\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "\n",
    "        print('loading_data...')\n",
    "        if slice_len is not None:\n",
    "            data = dataset[start_len:slice_len]\n",
    "        else:\n",
    "            data = dataset[start_len:]\n",
    "\n",
    "        print(\"tokenizinig_prefiltering...\")\n",
    "\n",
    "        filters = data.progress_map(lambda x: tokenizer.tokenize(x, padding = \"max_length\", max_length = max_length)[-1] == \"[PAD]\")\n",
    "        data = data[filters].reset_index(drop = True)\n",
    "\n",
    "\n",
    "        print(\"preprocessing...\")\n",
    "        multiprocessor.change_function(processor.preprocess_assign)\n",
    "        preprocessed_data_out = multiprocessor.do_series(data, num_cores = num_cores, pre_assign = True)\n",
    "        filters = CheckUtils.checkSeries(preprocessed_data_out,                                                       \n",
    "                                         isNan = True,                                                              \n",
    "                                         isEmpty = True,                                                              \n",
    "                                         isInf = False,                                                              \n",
    "                                         isNotInstance = None).map(lambda x: not x)\n",
    "        data = preprocessed_data_out[filters].reset_index(drop = True) # preprocessed\n",
    "        data.name = 'preprocessed'\n",
    "\n",
    "        print(\"spacy_parsing...\")\n",
    "        multiprocessor.change_function(processor.process_assign)\n",
    "        spacy_processed_out = multiprocessor.do_series(data, num_cores = num_cores, pre_assign = True)\n",
    "        spacy_processed_out.name = \"spacy_processed\"\n",
    "        data = pd.concat([data, spacy_processed_out], axis = 1) # preprocessed, spacy_processed\n",
    "        filters = CheckUtils.checkSeries(spacy_processed_out,                                                       \n",
    "                                         isNan = True,                                                              \n",
    "                                         isEmpty = True,                                                              \n",
    "                                         isInf = False,                                                              \n",
    "                                         isNotInstance = None).map(lambda x: not x)\n",
    "        data = data[filters].reset_index(drop = True)\n",
    "\n",
    "\n",
    "\n",
    "        print(\"extracting spacy attention mask...\")\n",
    "        multiprocessor.change_function(partial(Masking.extract_mask_assign, pure_dependency = True))\n",
    "        mask_out = multiprocessor.do_dataFrame(data, num_cores = num_cores, pre_assign = True)\n",
    "        mask_out.name = 'spacy_masking'\n",
    "        data = pd.concat([data, mask_out], axis = 1) # preprocessed, spacy_processed, masking\n",
    "        filters = CheckUtils.checkSeries(mask_out,                                                       \n",
    "                                         isNan = True,                                                              \n",
    "                                         isEmpty = True,                                                              \n",
    "                                         isInf = False,                                                              \n",
    "                                         isNotInstance = None).map(lambda x: not x)\n",
    "        data = data[filters].reset_index(drop = True)\n",
    "\n",
    "\n",
    "        print(\"expanding spacy attention mask to bert mask...\")\n",
    "        multiprocessor.change_function(partial(Masking.extend_mask_assign, tokenizer = tokenizer, max_length = max_length))\n",
    "        subword_masking = multiprocessor.do_dataFrame(data, num_cores = num_cores, pre_assign = True)\n",
    "        subword_masking.name = 'encoder_attention_mask'\n",
    "        data = pd.concat([data, subword_masking], axis = 1) # preprocessed, spacy_processed, masking, subword_masking\n",
    "        filters = CheckUtils.checkSeries(subword_masking,                                                       \n",
    "                                         isNan = True,                                                              \n",
    "                                         isEmpty = True,                                                              \n",
    "                                         isInf = False,                                                              \n",
    "                                         isNotInstance = None).map(lambda x: not x)\n",
    "        data = data[filters].reset_index(drop = True)\n",
    "\n",
    "        return data[['preprocessed', 'encoder_attention_mask']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "897ba1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DatasetWithCustomMasking(Dataset):\n",
    "    def __init__(self, data, tokenizer, *,  data_args, mask_args, tokenizer_config = None):\n",
    "        \n",
    "        self.data = list(data[data_args])        \n",
    "        self.masking = list(data[mask_args])\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        if tokenizer_config is not None:\n",
    "            self.padding = tokenizer_config['padding']\n",
    "            self.max_length = tokenizer_config['max_length']\n",
    "            self.return_tensors = tokenizer_config['return_tensors']\n",
    "            self.return_special_tokens_mask =  tokenizer_config['return_special_tokens_mask']\n",
    "            \n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.data) \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data[idx]\n",
    "        encoded = self.tokenizer(data,                         \n",
    "                         padding = self.padding,                        \n",
    "                         max_length = self.max_length,\n",
    "                         return_tensors = self.return_tensors,\n",
    "                         return_special_tokens_mask = self.return_special_tokens_mask\n",
    "                                 \n",
    "                                )\n",
    "        encoded.pop('token_type_ids')\n",
    "        encoded = {key: encoded[key].view(-1) for key in encoded}\n",
    "        \n",
    "        masking = self.masking[idx]\n",
    "        encoded['attention_mask'] = masking\n",
    "        \n",
    "        return encoded\n",
    "\n",
    "\n",
    "class TrainingFunctools():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def metric(eval_pred, func):\n",
    "        predictions, labels = eval_pred\n",
    "        predictions = np.argmax(predictions, axis = -1) # (batch, sequence lenagh, hidden_state)\n",
    "        filters = labels != -100\n",
    "\n",
    "        predictions = predictions[filters]\n",
    "        labels = labels[filters]\n",
    "        return func.compute(predictions = predictions, references = labels)\n",
    "\n",
    "    def setTrainer(*, \n",
    "          model,\n",
    "          tokenizer,\n",
    "          train_dataset,\n",
    "          eval_dataset,\n",
    "          data_collator,      \n",
    "          output_dir,\n",
    "          do_eval,\n",
    "          compute_metrics = metric,\n",
    "          eval_steps,\n",
    "          save_steps,\n",
    "          save_total_limit,\n",
    "          load_best_model_at_end,\n",
    "          save_evaluation_strategy,\n",
    "          metric_for_best_model,\n",
    "          greater_is_better,\n",
    "          num_epoch,\n",
    "          seed):\n",
    "        \n",
    "        args = TrainingArguments(do_train = True,    \n",
    "                         output_dir = output_dir,                         \n",
    "                         do_eval = do_eval,                        \n",
    "                         evaluation_strategy = save_evaluation_strategy, # necessary: change to step\n",
    "                         eval_steps = eval_steps, # necessary: set step\n",
    "                         save_steps = save_steps,\n",
    "                         save_strategy = save_evaluation_strategy,                         \n",
    "                         save_total_limit = save_total_limit,\n",
    "                         load_best_model_at_end = load_best_model_at_end, # necessary: EarlyStoppingCallBack하려면 True여야 함\n",
    "                         metric_for_best_model = metric_for_best_model,\n",
    "                         greater_is_better = greater_is_better, # necessary: higher metric results better performance # default = True when metric_for_best_model is set\n",
    "                         num_train_epochs = num_epoch,\n",
    "                         seed = seed,\n",
    "                         per_device_train_batch_size = 32,\n",
    "                         per_device_eval_batch_size = 32,\n",
    "                         learning_rate = 1e-6,\n",
    "                         weight_decay = 0.0001\n",
    "                             \n",
    "                            )\n",
    "\n",
    "        trainer = Trainer(model = model,\n",
    "                          args= args,\n",
    "                          train_dataset = train_dataset,\n",
    "                          eval_dataset = eval_dataset,\n",
    "                          tokenizer = tokenizer,\n",
    "                          data_collator = data_collator,\n",
    "                          compute_metrics = compute_metrics,\n",
    "                         )\n",
    "\n",
    "        return trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecb940d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_config = {\"shuffle\": True, \"batch_size\": 32}\n",
    "tokenizer_config = {'padding': 'max_length', \"max_length\": 128, 'truncation' : True, 'return_tensors' : 'pt', 'return_special_tokens_mask': True}\n",
    "optimizer_config = {'type': 'adam', 'lr': 1e-6, \"weight_decay\": 0.001, 'eps' : 1e-08} # default except learning rate(1e-3)\n",
    "loss_config = {'type': 'crossEntropyLoss'}\n",
    "\n",
    "def save_data(*,\n",
    "         dataset = None,\n",
    "         load_dataset = False,\n",
    "         trainer_start_len = None,\n",
    "         trainer_slice_len = None,\n",
    "         eval_slice_len = None,\n",
    "         test_slice_len = None,\n",
    "         num_cores = 40,\n",
    "         model_name = \"google-bert/bert-base-uncased\",\n",
    "         dataloader_config = dataloader_config,\n",
    "         tokenizer_config = tokenizer_config,\n",
    "         optimizer_config = optimizer_config,\n",
    "         loss_config =loss_config,\n",
    "         checkpoint_output = \"./checkpoint_output\",\n",
    "         \n",
    "        ):\n",
    "    \n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForMaskedLM.from_config(config = AutoConfig.from_pretrained(model_name))\n",
    "\n",
    "    \n",
    "    data = DatasetProducer()\n",
    "    data.train_data, data.eval_data, data.test_data = data.load_dataset()\n",
    "    data.train_data = data.load_data_with_custom_mask(dataset = data.train_data, \n",
    "                                                 slice_len = trainer_slice_len, \n",
    "                                                 start_len = trainer_start_len,\n",
    "                                                 num_cores = num_cores)  \n",
    "    with open(f\"train_data_{trainer_slice_len}\", \"wb\") as fw:\n",
    "        pickle.dump(data.train_data, fw)\n",
    "    \n",
    "\n",
    "    \n",
    "    data.eval_data = data.load_data_with_custom_mask(dataset = data.eval_data, \n",
    "                                                slice_len = eval_slice_len, \n",
    "                                                num_cores = num_cores)\n",
    "    data.test_data = load_data_with_custom_mask(dataset = data.test_data, \n",
    "                                                slice_len = test_slice_len, \n",
    "                                                num_cores = num_cores)\n",
    "\n",
    "    \n",
    "    with open(\"eval_data\", \"wb\") as fw:\n",
    "        pickle.dump(data.eval_data, fw)\n",
    "        \n",
    "#     data.train_dataset = DatasetWithCustomMasking(data.train_data, tokenizer,                          \n",
    "#                                              data_args = 'preprocessed',                          \n",
    "#                                              mask_args = \"encoder_attention_mask\",                       \n",
    "#                                              tokenizer_config = tokenizer_config)\n",
    "#     data.eval_dataset = DatasetWithCustomMasking(data.eval_data, tokenizer, \n",
    "#                                             data_args = 'preprocessed',                                                   \n",
    "#                                             mask_args = \"encoder_attention_mask\",                                                \n",
    "#                                             tokenizer_config = tokenizer_config)\n",
    "# # #     data.test_dataset = DatasetWithCustomMasking(data.test_dataset, tokenizer, \n",
    "# # #                                             data_args = 'preprocessed',                                                   \n",
    "# # #                                             mask_args = \"encoder_attention_mask\",                                                \n",
    "# # #                                             tokenizer_config = tokenizer_config)\n",
    "    \n",
    "    \n",
    "#     data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
    "#     accuracy = evaluate.load('accuracy')\n",
    "    \n",
    "#     output_dir = str(Path(checkpoint_output).resolve())\n",
    "    \n",
    "#     trainer = TrainingFunctools.setTrainer(model = model,     \n",
    "#                     tokenizer = tokenizer,     \n",
    "#                     train_dataset = data.train_dataset,   \n",
    "#                     compute_metrics = partial(TrainingFunctools.metric, func = accuracy), \n",
    "#                     do_eval = True,     \n",
    "#                     output_dir = output_dir,\n",
    "#                     eval_steps = 1,    \n",
    "#                     save_steps = 1,\n",
    "#                     save_total_limit = 3,                \n",
    "#                     load_best_model_at_end = True,                \n",
    "#                     save_evaluation_strategy = \"steps\",                \n",
    "#                     metric_for_best_model = \"eval_accuracy\",    \n",
    "#                     greater_is_better = True,\n",
    "#                     train_batch_size = 32,                \n",
    "#                     eval_batch_size = 32,                \n",
    "#                     num_epoch = 20,                \n",
    "#                     seed = 42,\n",
    "#                     eval_dataset = data.eval_dataset,     \n",
    "#                     data_collator = data_collator,    \n",
    "#                     save_only_model = True\n",
    "#                    )\n",
    "\n",
    "#     trainer.train()\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c289e5f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae5fcf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_tensor(tensor1, tensor2, index):\n",
    "    t = torch.tensor(tensor1, requires_grad = False)\n",
    "    t[...,:index,:index] = tensor2\n",
    "    return t\n",
    "\n",
    "def append_random_attention(df, tokenizer, tokenizer_config):\n",
    "    \n",
    "    \n",
    "    padding = tokenizer_config['padding']\n",
    "    max_length = tokenizer_config['max_length']\n",
    "    return_tensors  = tokenizer_config['return_tensors']\n",
    "    df_sentence = df['preprocessed']\n",
    "    \n",
    "    df_len = df_sentence.progress_map(lambda x: len(tokenizer.tokenize(x)))\n",
    "    df_len.name = \"real_token_length\"\n",
    "    df_whole = df_len.progress_map(lambda x: x ** 2)\n",
    "    \n",
    "    df_temp = pd.concat([df['encoder_attention_mask'].copy(), df_len], axis = 1)\n",
    "    df_ones = df_temp.progress_apply(lambda x: x['encoder_attention_mask'][:x['real_token_length']][:x['real_token_length']].sum(), axis = 1)\n",
    "    \n",
    "    probs = df_ones.sum() / df_whole.sum()\n",
    "    \n",
    "    df_random_attention = df_len.progress_map(lambda x: torch.bernoulli(torch.full((x,x), probs)) )\n",
    "    df_random_attention.name = \"real\"\n",
    "    df_attention_whole = df_len.progress_map(lambda x: torch.zeros(max_length, max_length))\n",
    "    df_attention_whole.name = \"whole\"\n",
    "    \n",
    "    df_attention = pd.concat([df_random_attention, df_attention_whole, df_len], axis = 1)\n",
    "    out = df_attention.progress_apply(lambda x: fill_tensor(x['whole'], x['real'], x['real_token_length']), axis = 1)\n",
    "    out.name = \"encoder_attention_mask\"\n",
    "    \n",
    "    return pd.concat([df_sentence, out], axis = 1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "02b67261",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_config = {\"shuffle\": True, \"batch_size\": 32}\n",
    "tokenizer_config = {'padding': 'max_length', \"max_length\": 128, 'truncation' : True, 'return_tensors' : 'pt', 'return_special_tokens_mask': True}\n",
    "optimizer_config = {'type': 'adam', 'lr': 1e-6, \"weight_decay\": 0.001, 'eps' : 1e-08} # default except learning rate(1e-3)\n",
    "loss_config = {'type': 'crossEntropyLoss'}\n",
    "\n",
    "def main(*,\n",
    "         dataset = None,\n",
    "         load_dataset = False,\n",
    "         trainer_start_len = None,\n",
    "         trainer_slice_len = None,\n",
    "         eval_slice_len = None,\n",
    "         eval_start_len = None,\n",
    "         test_slice_len = None,\n",
    "         num_cores = 20,\n",
    "         model_name = \"google-bert/bert-base-uncased\",\n",
    "         dataloader_config = dataloader_config,\n",
    "         tokenizer_config = tokenizer_config,\n",
    "         optimizer_config = optimizer_config,\n",
    "         loss_config =loss_config,\n",
    "         checkpoint_output = \"./checkpoint_output_{}\",\n",
    "         train_type \n",
    "         \n",
    "        ):\n",
    "    \n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForMaskedLM.from_config(config = AutoConfig.from_pretrained(model_name))\n",
    "    checkpoint_output = checkpoint_output.format(train_type)\n",
    "    print(checkpoint_output)\n",
    "    \n",
    "    data = DatasetProducer()\n",
    "#     data.train_data, data.eval_data, data.test_data = data.load_dataset()\n",
    "# #     data.train_data = data.load_data_with_custom_mask(dataset = data.train_data, \n",
    "# #                                                  slice_len = trainer_slice_len, \n",
    "# #                                                  start_len = trainer_start_len,\n",
    "# #                                                  num_cores = num_cores)  \n",
    "# #     with open(f\"train_data_{trainer_slice_len}\", \"wb\") as fw:\n",
    "# #         pickle.dump(data.train_data, fw)\n",
    "    \n",
    "\n",
    "    \n",
    "#     data.eval_data = data.load_data_with_custom_mask(dataset = data.eval_data, \n",
    "#                                                 slice_len = eval_slice_len, \n",
    "#                                                 start_len = eval_start_len,\n",
    "#                                                 num_cores = num_cores)\n",
    "#     with open(\"eval_data\", \"wb\") as fw:\n",
    "#         pickle.dump(data.eval_data, fw)\n",
    "        \n",
    "#     data.test_data = load_data_with_custom_mask(dataset = data.test_data, \n",
    "#                                                 slice_len = test_slice_len, \n",
    "#                                                 num_cores = num_cores)\n",
    "\n",
    "    data_pickle = []\n",
    "    for i in range(20000, 360000, 20000):\n",
    "        with open(f\"train_data/train_data_{i}\", \"rb\") as fr:\n",
    "            print(f\"load {i}-th data pickles\")\n",
    "            data_pickle.append(pickle.load(fr))\n",
    "    \n",
    "    data.train_data = pd.concat(data_pickle, axis = 0)\n",
    "    \n",
    "    with open(f\"eval_data/eval_data\", \"rb\") as fr:\n",
    "        data.eval_data = pickle.load(fr)\n",
    "        \n",
    "    print(len(data.train_data), len(data.eval_data))\n",
    "   \n",
    "    if train_type == \"dependency\":\n",
    "        pass\n",
    "    \n",
    "    if train_type == \"random\":\n",
    "        data.train_data = append_random_attention(data.train_data, tokenizer, tokenizer_config)\n",
    "        data.eval_data = append_random_attention(data.eval_data, tokenizer, tokenizer_config)\n",
    "    \n",
    "    \n",
    "        \n",
    "    data.train_dataset = DatasetWithCustomMasking(data.train_data, tokenizer,                          \n",
    "                                             data_args = 'preprocessed',                          \n",
    "                                             mask_args = \"encoder_attention_mask\",                       \n",
    "                                             tokenizer_config = tokenizer_config)\n",
    "    data.eval_dataset = DatasetWithCustomMasking(data.eval_data, tokenizer, \n",
    "                                            data_args = 'preprocessed',                                                   \n",
    "                                            mask_args = \"encoder_attention_mask\",                                                \n",
    "                                            tokenizer_config = tokenizer_config)\n",
    "# #     data.test_dataset = DatasetWithCustomMasking(data.test_dataset, tokenizer, \n",
    "# #                                             data_args = 'preprocessed',                                                   \n",
    "# #                                             mask_args = \"encoder_attention_mask\",                                                \n",
    "# #                                             tokenizer_config = tokenizer_config)\n",
    "    \n",
    "\n",
    "#     append_random_attention(data.train_dataset)\n",
    "#     append_random_attention(data.eval_dataset)\n",
    "    \n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
    "    accuracy = evaluate.load('accuracy')\n",
    "    \n",
    "    output_dir = str(Path(checkpoint_output).resolve())\n",
    "    \n",
    "    trainer = TrainingFunctools.setTrainer(model = model,     \n",
    "                    tokenizer = tokenizer,     \n",
    "                    train_dataset = data.train_dataset,   \n",
    "                    compute_metrics = partial(TrainingFunctools.metric, func = accuracy), \n",
    "                    do_eval = True,     \n",
    "                    output_dir = output_dir,\n",
    "                    eval_steps = 500,    \n",
    "                    save_steps = 500,\n",
    "                    save_total_limit = 3,                \n",
    "                    load_best_model_at_end = True,                \n",
    "                    save_evaluation_strategy = \"steps\",                \n",
    "                    metric_for_best_model = \"eval_accuracy\",    \n",
    "                    greater_is_better = True,              \n",
    "                    num_epoch = 200,                \n",
    "                    seed = 42,\n",
    "                    eval_dataset = data.eval_dataset,     \n",
    "                    data_collator = data_collator,    \n",
    "                   )\n",
    "\n",
    "    trainer.train()\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed503876",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath(os.path.dirname(\"data_generation/\")))\n",
    "\n",
    "from utils import *\n",
    "# from data_generation.generation_projects.examples import adjunct_island\n",
    "from utils import data_generator\n",
    "from utils.constituent_building import *\n",
    "from utils.conjugate import *\n",
    "from utils.randomize import choice\n",
    "from utils.vocab_sets_dynamic import *\n",
    "\n",
    "\n",
    "class AttentionTestDataGenerator(data_generator.BenchmarkGenerator):\n",
    "    def __init__(self, metadata_dict):\n",
    "        super().__init__(**metadata_dict)\n",
    "        self.all_ing_transitives = np.intersect1d(get_all_transitive_verbs(),\n",
    "                                                 get_all_ing_verbs())\n",
    "        self.adverbs = [\"before\", \"while\", \"after\", \"without\"]\n",
    "    \n",
    "    def sample(self):\n",
    "        # Jean invited John         and      every friend\n",
    "        # Subj V_mat   obj1         conj     obj2  \n",
    "        # Jean invited every friend and      John\n",
    "        # Subj V_mat   obj1         conj     obj2  \n",
    "        # Jean invited John         and      every female\n",
    "        # Subj V_mat   obj1         conj     obj2  \n",
    "        # Jean invited every female and      John\n",
    "        # Subj V_mat   obj1         conj     obj2  \n",
    "        \n",
    "        V_mat = choice(get_all_non_finite_transitive_verbs())\n",
    "        \n",
    "        subj = N_to_DP_mutate(choice(get_matches_of(V_mat, \"arg_1\", get_all_common_nouns()))) # propernoun\n",
    "        obj1 = N_to_DP_mutate(choice(get_matches_of(V_mat, \"arg_2\", get_all_common_nouns())), allow_quantifiers = False, avoid = subj) # \n",
    "        obj2_quant = N_to_DP_mutate(choice(get_matches_of(V_mat, \"arg_2\", get_all_nouns())), determiner = False)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "def generate_sample(metadata = None, path = None):\n",
    "    \n",
    "    type_coersion_conjuction = {\"field\": \"semantics\",\n",
    "                                \"linguistics\": \"type_coersion\",\n",
    "                                \"uid\": \"coersion_conjunction\",\n",
    "                               \"simple_lm_method\": True,\n",
    "                               \"one_prefix_method\": False,\n",
    "                               \"two_prefix_method\": False,\n",
    "                               \"lexically_identical\": True,\n",
    "                               }\n",
    "    type_coersion_conjunction_path = \"./temp/{}.jsonl\"\n",
    "\n",
    "    relpath = type_coersion_conjunction_path.format(type_coersion_conjuction['uid'])\n",
    "    abspath = str(Path(relpath).resolve())\n",
    "\n",
    "    generator = AttentionTestDataGenerator(type_coersion_conjuction)\n",
    "    generator.generate_paradigm(absolute_path = type_coersion_conjunction_path)\n",
    "\n",
    "    \n",
    "#     lst = []\n",
    "#     with jsonlines.open(abspath , \"r\") as f:\n",
    "#         for line in f.iter():\n",
    "#             lst.append(json.loads(str(i, \"utf-8\")))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d550416e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98f921c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2841a9f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248945a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41df7b58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c46b899",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3d2371",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "37eff71f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hyohyeongjang/.local/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./checkpoint_output_dependency\n",
      "load 20000-th data pickles\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 28\u001b[0m\n\u001b[1;32m     23\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTOKENIZERS_PARALLELISM\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfalse\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m#     out = main(trainer_start_len = args.startnum,\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m#               trainer_slice_len = args.num)\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m     td, ed \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_cores\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdependency\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[30], line 57\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(dataset, load_dataset, trainer_start_len, trainer_slice_len, eval_slice_len, eval_start_len, test_slice_len, num_cores, model_name, dataloader_config, tokenizer_config, optimizer_config, loss_config, checkpoint_output, train_type)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_data/train_data_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fr:\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-th data pickles\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 57\u001b[0m         data_pickle\u001b[38;5;241m.\u001b[39mappend(\u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfr\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     59\u001b[0m data\u001b[38;5;241m.\u001b[39mtrain_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(data_pickle, axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_data/eval_data\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fr:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/storage.py:381\u001b[0m, in \u001b[0;36m_load_from_bytes\u001b[0;34m(b)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_load_from_bytes\u001b[39m(b):\n\u001b[0;32m--> 381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBytesIO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/serialization.py:1040\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1039\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1040\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_legacy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/serialization.py:1281\u001b[0m, in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1279\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m deserialized_objects\n\u001b[1;32m   1280\u001b[0m typed_storage \u001b[38;5;241m=\u001b[39m deserialized_objects[key]\n\u001b[0;32m-> 1281\u001b[0m \u001b[43mtyped_storage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_untyped_storage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_from_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1282\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf_should_read_directly\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_element_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtyped_storage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m offset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1285\u001b[0m     offset \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mtell()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "#     parser.add_argument('--num', default = None, type = int)\n",
    "#     parser.add_argument('--startnum', default = None, type = int)\n",
    "\n",
    "    parser.add_argument('--train_type', default=\"dependency\", type=str, required=True, help='Count of derivatives.')\n",
    "#    # parser.add_argument('--lexicon', default=None, type=str, required=True, help='Lexicon setting')\n",
    "#    # parser.add_argument('--mode', default=None, type=str, required=True, help='Affix type.')\n",
    "#    # parser.add_argument('--batch_size', default=None, type=int, required=True, help='Batch size.')\n",
    "#    # parser.add_argument('--lr', default=None, type=float, required=True, help='Learning rate.')\n",
    "#    # parser.add_argument('--n_epochs', default=None, type=int, required=True, help='Number of epochs.')\n",
    "#    # parser.add_argument('--cuda', default=None, type=int, required=True, help='Selected CUDA.')\n",
    "#    # parser.add_argument('--freeze', default=False, action='store_true', help='Freeze BERT parameters.')\n",
    "#    # parser.add_argument('--inpath', default=None, type=str, required=True, help='Count of derivatives.')\n",
    "#    # parser.add_argument('--data', default=None, type=str, required=True, help='Count of derivatives.')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    import os\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "    \n",
    "#     out = main(trainer_start_len = args.startnum,\n",
    "#               trainer_slice_len = args.num)\n",
    "    td, ed = main(num_cores = 1, train_type = args.train_type)\n",
    "#     out = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015c3efd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec93ab59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bbef42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115d93fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f98142",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f518d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5579ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dcab81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99fc915",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d716067e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb49059",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7510844d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d9b218",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1a9ee0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c056d83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b50fdc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b5a95c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaaccab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4f1ee1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87337890",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80979e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c54ef4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073313c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c8094c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb624d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4f9cad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff856077",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84f1ec5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6babdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a090e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f197fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70c975a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83711dae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3170309a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd58da2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "# parser = argparse.ArgumentParser()\n",
    "\n",
    "# parser.add_argument('--do_train', default = False, action='store_true')\n",
    "\n",
    "# # parser.add_argument('--count', default=None, type=int, required=True, help='Count of derivatives.')\n",
    "# # parser.add_argument('--lexicon', default=None, type=str, required=True, help='Lexicon setting')\n",
    "# # parser.add_argument('--mode', default=None, type=str, required=True, help='Affix type.')\n",
    "# # parser.add_argument('--batch_size', default=None, type=int, required=True, help='Batch size.')\n",
    "# # parser.add_argument('--lr', default=None, type=float, required=True, help='Learning rate.')\n",
    "# # parser.add_argument('--n_epochs', default=None, type=int, required=True, help='Number of epochs.')\n",
    "# # parser.add_argument('--cuda', default=None, type=int, required=True, help='Selected CUDA.')\n",
    "# # parser.add_argument('--freeze', default=False, action='store_true', help='Freeze BERT parameters.')\n",
    "# # parser.add_argument('--inpath', default=None, type=str, required=True, help='Count of derivatives.')\n",
    "# # parser.add_argument('--data', default=None, type=str, required=True, help='Count of derivatives.')\n",
    "\n",
    "# args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "843c86ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming 'tokens' is already defined as a DataFrame or array\n",
    "# df = pd.DataFrame(out.loc[0].spacy_masking)\n",
    "\n",
    "# plt.pcolor(df)\n",
    "# plt.xticks(np.arange(0.5, len(df.columns), 1), df.columns)\n",
    "# plt.yticks(np.arange(0.5, len(df.index), 1), df.index)\n",
    "# plt.gca().invert_yaxis()  # Invert the y-axis to move (0,0) to the upper left\n",
    "# plt.title('Heatmap by plt.pcolor()', fontsize=20)\n",
    "# # plt.xlabel('Year', fontsize=14)\n",
    "# # plt.ylabel('Month', fontsize=14)\n",
    "# plt.colorbar()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fac835",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
